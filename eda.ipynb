{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "# Make sure sorted by date\n",
    "df = df.sort_values(by='date')\n",
    "sub = sub.sort_values(by='date')\n",
    "\n",
    "# Format date as date\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
    "sub[\"date\"] = pd.to_datetime(sub[\"date\"]).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is going to be a common problem to fix, but tracking is doubled 9/27/2017 and 6/12/2018\n",
    "\n",
    "def fix_double_tracking(df, col, date_col=\"date\"):\n",
    "    df.loc[(df[date_col] <= pd.to_datetime(\"2018-06-12\").date()) & (df[date_col] >= pd.to_datetime(\"2017-09-27\").date()), col] = \\\n",
    "    df.loc[(df[date_col] <= pd.to_datetime(\"2018-06-12\").date()) & (df[date_col] >= pd.to_datetime(\"2017-09-27\").date()), col] / 2\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleep_hours between 9/27/2017 and 6/12/2018 is doubled and needs to be divided by 2\n",
    "df = fix_double_tracking(df, \"sleep_hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sleep_hours.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing dates (for time series)\n",
    "\n",
    "start_date = df[\"date\"].min()\n",
    "print(f\"Start date: {start_date}\")\n",
    "\n",
    "end_date = df[\"date\"].max()\n",
    "print(f\"End date: {end_date}\")\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "date_range = pd.DataFrame({\"date\": date_range})\n",
    "date_range[\"date\"] = date_range[\"date\"].dt.date\n",
    "\n",
    "df = date_range.merge(df, on=\"date\", how=\"left\")\n",
    "print('missing days: ', df.sleep_hours.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward fill the missing values/targets\n",
    "df[\"sleep_hours\"] = df[\"sleep_hours\"].fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot df_sleep.sleep_hours as dots by df_sleep.date\n",
    "df.plot.scatter(x='date', y='sleep_hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median of sleep_hours\n",
    "med = df.sleep_hours.median()\n",
    "print(f\"Median sleep hours: {med}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_median = sub.copy()\n",
    "sub_median[\"sleep_hours\"] = med\n",
    "sub_median.to_csv(\"./submissions/submission_median.csv\", index=False)\n",
    "sub_median.head()\n",
    "\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_median.csv -m \"Trying just all median\"\n",
    "# Score : 0.67685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median of sleep_hours\n",
    "mean = df.sleep_hours.mean()\n",
    "print(f\"Median sleep hours: {mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_mean = sub.copy()\n",
    "sub_mean[\"sleep_hours\"] = mean\n",
    "sub_mean.to_csv(\"./submissions/submission_mean.csv\", index=False)\n",
    "sub_mean.head()\n",
    "\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_mean.csv -m \"Trying just all mean\"\n",
    "# Score: 0.67396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "### Time series Data ###\n",
    "\n",
    "df[\"day_of_week\"] = pd.to_datetime(df[\"date\"]).dt.dayofweek # Day of the week\n",
    "df[\"month\"] = pd.to_datetime(df[\"date\"]).dt.month # Month\n",
    "df[\"is_weekend\"] = df[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0) # is_weekend\n",
    "\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start=\"2014-01-01\", end=\"2023-12-31\") \n",
    "df[\"is_holiday\"] = df[\"date\"].apply(lambda x: 1 if x in holidays else 0) # is_holiday\n",
    "df[\"is_workday\"] = 1-(df[\"is_weekend\"] + df[\"is_holiday\"]) # is_workday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submissions by median by DOW\n",
    "sub_median_group = sub.copy()\n",
    "med = df.groupby(\"day_of_week\")[\"sleep_hours\"].median()\n",
    "sub_median_group[\"day_of_week\"] = pd.to_datetime(sub_median_group[\"date\"]).dt.dayofweek\n",
    "sub_median_group[\"sleep_hours\"] = sub_median_group[\"day_of_week\"].map(med)\n",
    "sub_median_group = sub_median_group.drop(\"day_of_week\", axis=1)\n",
    "sub_median_group.to_csv(\"./submissions/submission_median_group.csv\", index=False)\n",
    "sub_median_group.head()\n",
    "\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_median_group.csv -m \"Trying all median by DOW\"\n",
    "# Score: 0.67406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submissions by median by DOW\n",
    "sub_mean_group = sub.copy()\n",
    "mean = df.groupby(\"day_of_week\")[\"sleep_hours\"].mean()\n",
    "sub_mean_group[\"day_of_week\"] = pd.to_datetime(sub_mean_group[\"date\"]).dt.dayofweek\n",
    "sub_mean_group[\"sleep_hours\"] = sub_mean_group[\"day_of_week\"].map(mean)\n",
    "sub_mean_group = sub_mean_group.drop(\"day_of_week\", axis=1)\n",
    "sub_mean_group.to_csv(\"./submissions/submission_mean_group.csv\", index=False)\n",
    "sub_mean_group.head()\n",
    "\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_mean_group.csv -m \"Trying all mean by DOW\"\n",
    "# Score: ________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission by median by DOW and and is_workday\n",
    "sub_median_group_workday = sub.copy()\n",
    "med = df.groupby([\"day_of_week\", \"is_workday\"])[\"sleep_hours\"].median()\n",
    "sub_median_group_workday[\"day_of_week\"] = pd.to_datetime(sub_median_group_workday[\"date\"]).dt.dayofweek\n",
    "sub_median_group_workday[\"is_workday\"] = 1-(sub_median_group_workday[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0) + sub_median_group_workday[\"date\"].apply(lambda x: 1 if x in holidays else 0))\n",
    "sub_median_group_workday[\"sleep_hours\"] = sub_median_group_workday[[\"day_of_week\", \"is_workday\"]].apply(lambda x: med[x[0]][x[1]], axis=1)\n",
    "sub_median_group_workday = sub_median_group_workday.drop([\"day_of_week\", \"is_workday\"], axis=1)\n",
    "sub_median_group_workday.to_csv(\"./submissions/submission_median_group_workday.csv\", index=False)\n",
    "sub_median_group_workday.head()\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_median_group_workday.csv -m \"Trying all median by DOW and is_workday\"\n",
    "# Score: __________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission by mean by DOW and and is_workday\n",
    "sub_mean_group_workday = sub.copy()\n",
    "mean = df.groupby([\"day_of_week\", \"is_workday\"])[\"sleep_hours\"].mean()\n",
    "sub_mean_group_workday[\"day_of_week\"] = pd.to_datetime(sub_mean_group_workday[\"date\"]).dt.dayofweek\n",
    "sub_mean_group_workday[\"is_workday\"] = 1-(sub_mean_group_workday[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0) + sub_mean_group_workday[\"date\"].apply(lambda x: 1 if x in holidays else 0))\n",
    "sub_mean_group_workday[\"sleep_hours\"] = sub_mean_group_workday[[\"day_of_week\", \"is_workday\"]].apply(lambda x: mean[x[0]][x[1]], axis=1)\n",
    "sub_mean_group_workday = sub_mean_group_workday.drop([\"day_of_week\", \"is_workday\"], axis=1)\n",
    "sub_mean_group_workday.to_csv(\"./submissions/submission_mean_group_workday.csv\", index=False)\n",
    "sub_mean_group_workday.head()\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_mean_group_workday.csv -m \"Trying all mean by DOW and is_workday\"\n",
    "# Score: ________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "csv_files = glob(\"./data/xml_export/*.csv\")\n",
    "\n",
    "# Keep what files have ~80% of the dates in the training data\n",
    "unique_dates = set(df[\"date\"])\n",
    "\n",
    "match_threshold = 0.75 # Threshold for matching\n",
    "\n",
    "def intersection_ratio(set1, set2):\n",
    "    return len(set1 & set2) / len(set1)\n",
    "\n",
    "matching_csvs = []\n",
    "for csv_file in csv_files:\n",
    "    csv_df = pd.read_csv(csv_file)  \n",
    "    try: \n",
    "        csv_df[\"date\"] = pd.to_datetime(csv_df[\"startDate\"]).dt.date\n",
    "    except:\n",
    "        csv_df[\"date\"] = pd.to_datetime(csv_df[\"dateComponents\"]).dt.date # Some use dateComponents instead of startDate\n",
    "\n",
    "    unique_creation_dates = set(csv_df)\n",
    "\n",
    "    ratio = intersection_ratio(unique_dates, unique_creation_dates)\n",
    "\n",
    "    if ratio >= match_threshold:\n",
    "        matching_csvs.append(csv_file)\n",
    "\n",
    "print(\"CSV files with at least 75% of the 'dates' from the original DataFrame:\")\n",
    "for matching_csv in matching_csvs:\n",
    "    print(matching_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_df.loc[csv_df[\"date\"] >= pd.to_datetime(\"2021-01-01\").date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [\"test1\", \"test2\", \"test3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets worth exploring\n",
    "basal_energy_burned = pd.read_csv('./data/xml_export/BasalEnergyBurned.csv', low_memory=False)\n",
    "body_mass = pd.read_csv('./data/xml_export/BodyMass.csv', low_memory=False)\n",
    "flights_climbed = pd.read_csv('./data/xml_export/FlightsClimbed.csv', low_memory=False)\n",
    "step_count = pd.read_csv('./data/xml_export/StepCount.csv', low_memory=False)\n",
    "body_mass_index = pd.read_csv('./data/xml_export/BodyMassIndex.csv', low_memory=False)\n",
    "distance_walking_running = pd.read_csv('./data/xml_export/DistanceWalkingRunning.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_xml_output(path):\n",
    "    # Import the csv\n",
    "    csv_df = pd.read_csv(path, low_memory=False)\n",
    "    base_name = os.path.basename(path).split(\".\")[0]\n",
    "    \n",
    "    # BodyMassIndex we want to avg not sum\n",
    "    if base_name == \"BodyMassIndex\":\n",
    "        agg_func = \"mean\"\n",
    "    else:\n",
    "        agg_func = \"sum\"\n",
    "    \n",
    "    # Convert startDate and endDate columns to datetime objects\n",
    "    csv_df[\"startDate\"] = pd.to_datetime(csv_df[\"startDate\"]).dt.tz_convert (\"US/Eastern\")\n",
    "    csv_df[\"endDate\"] = pd.to_datetime(csv_df[\"endDate\"]).dt.tz_convert (\"US/Eastern\")\n",
    "    # Create date & time column\n",
    "    csv_df[\"date\"] = pd.to_datetime(csv_df[\"startDate\"]).dt.date\n",
    "    csv_df[\"time\"] = pd.to_datetime(csv_df[\"startDate\"]).dt.time\n",
    "    \n",
    "    # Group by date and perform aggregations\n",
    "    csv_df = csv_df.groupby(\"date\").agg(\n",
    "        max_start_time=pd.NamedAgg(column=\"startDate\", aggfunc=\"max\"),\n",
    "        min_start_time=pd.NamedAgg(column=\"startDate\", aggfunc=\"min\"),\n",
    "        max_end_time=pd.NamedAgg(column=\"endDate\", aggfunc=\"max\"),\n",
    "        min_end_time=pd.NamedAgg(column=\"endDate\", aggfunc=\"min\"),\n",
    "        value_sum=pd.NamedAgg(column=\"value\", aggfunc=agg_func)\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Check if the dates match between min_start_time and max_end_time\n",
    "    csv_df[\"dates_match\"] = csv_df.apply(\n",
    "        lambda row: row[\"min_start_time\"].date() == row[\"max_end_time\"].date(), axis=1)\n",
    "    csv_df = csv_df.rename(columns={\"value_sum\": base_name})\n",
    "    \n",
    "    # Assert if dates match in all rows\n",
    "    # assert csv_df[\"dates_match\"].all(), \"Dates do not match in some rows\"\n",
    "    \n",
    "    # Trigonomic Hours\n",
    "    csv_df[base_name+\"_mx_st_hr_sin\"] = np.sin(2 * np.pi * csv_df[\"max_start_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mx_st_hr_cos\"] = np.cos(2 * np.pi * csv_df[\"max_start_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mn_st_hr_sin\"] = np.sin(2 * np.pi * csv_df[\"min_start_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mn_st_hr_cos\"] = np.cos(2 * np.pi * csv_df[\"min_start_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mx_et_hr_sin\"] = np.sin(2 * np.pi * csv_df[\"max_end_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mx_et_hr_cos\"] = np.cos(2 * np.pi * csv_df[\"max_end_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mn_et_hr_sin\"] = np.sin(2 * np.pi * csv_df[\"min_end_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mn_et_hr_cos\"] = np.cos(2 * np.pi * csv_df[\"min_end_time\"].dt.hour / 24) \n",
    "    \n",
    "    #! HIGHLY ILLEGAL FEATURE??? (Step Count basically calculates sleep time)\n",
    "    # Hours between startDate and next startDate (lag = -1)\n",
    "    csv_df[base_name+\"_hours_between\"] = (csv_df[\"min_start_time\"].shift(-1) - csv_df[\"max_start_time\"]).dt.total_seconds() / 3600\n",
    "    \n",
    "    #Fix double tracking\n",
    "    csv_df = fix_double_tracking(csv_df, base_name)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    # csv_df = csv_df.drop(columns=[\"max_start_time\", \"min_start_time\", \"max_end_time\", \"min_end_time\", \"dates_match\"])\n",
    "\n",
    "    return csv_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basal Energy Burned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basal_energy_burned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value is the only useful feature\n",
    "# Group by date and sum (set alias to basal_energy_burned)\n",
    "\n",
    "# NOTE: Summing assumes duplicate tracking is not occuring between devices\n",
    "\n",
    "basal_energy_burned = parse_xml_output(\"./data/xml_export/BasalEnergyBurned.csv\")\n",
    "basal_energy_burned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "basal_energy_burned.BasalEnergyBurned.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basal_energy_burned.plot.scatter(x='date', y='BasalEnergyBurned')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Rob got a better tracker or became more serious about racking in 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass[\"date\"] = pd.to_datetime(body_mass[\"startDate\"]).dt.date\n",
    "body_mass = body_mass.groupby(\"date\").sum().reset_index()\n",
    "body_mass = body_mass.rename(columns={\"value\": \"body_mass\"})\n",
    "body_mass = fix_double_tracking(body_mass, \"body_mass\")\n",
    "body_mass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "body_mass.body_mass.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass.plot.scatter(x='date', y='body_mass')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flights Climbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_climbed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_climbed[\"date\"] = pd.to_datetime(flights_climbed[\"startDate\"]).dt.date\n",
    "flights_climbed = flights_climbed.groupby(\"date\").sum().reset_index()\n",
    "flights_climbed = flights_climbed.rename(columns={\"value\": \"flights_climbed\"})\n",
    "flights_climbed = fix_double_tracking(flights_climbed, \"flights_climbed\")\n",
    "flights_climbed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "flights_climbed.flights_climbed.hist()\n",
    "\n",
    "# Definitely lognormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_climbed.plot.scatter(x='date', y='flights_climbed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very weird data. Will have to explore further. Possible duplication?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count[\"date\"] = pd.to_datetime(step_count[\"startDate\"]).dt.date\n",
    "step_count = step_count.groupby(\"date\").sum().reset_index()\n",
    "step_count = step_count.rename(columns={\"value\": \"step_count\"})\n",
    "step_count = fix_double_tracking(step_count, \"step_count\")\n",
    "step_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "step_count.step_count.hist()\n",
    "\n",
    "#kind of lognormal but not quite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count.plot.scatter(x='date', y='step_count')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body Mass Index\n",
    "This is a slow changing variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass_index[\"date\"] = pd.to_datetime(body_mass_index[\"startDate\"]).dt.date\n",
    "body_mass_index = body_mass_index.groupby(\"date\").mean(numeric_only =True).reset_index() #! MEAN here\n",
    "body_mass_index = body_mass_index.rename(columns={\"value\": \"body_mass_index\"})\n",
    "body_mass_index = body_mass_index.drop(columns=[\"sourceVersion\", \"device\"], axis=1)\n",
    "body_mass_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "body_mass_index.body_mass_index.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass_index.plot.scatter(x='date', y='body_mass_index')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not measured frequently in 2012-2015"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Walking/Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_walking_running.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_walking_running = parse_xml_output(\"./data/xml_export/DistanceWalkingRunning.csv\")\n",
    "distance_walking_running.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "distance_walking_running.DistanceWalkingRunning.hist()\n",
    "\n",
    "# Lognormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_walking_running.plot.scatter(x='date', y='DistanceWalkingRunning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the hour and minute information from the datetime column and convert it to minutes\n",
    "distance_walking_running['time_minutes'] = distance_walking_running['min_start_time'].dt.hour * 60 + distance_walking_running['min_start_time'].dt.minute\n",
    "\n",
    "# Define the bin size (in minutes)\n",
    "bin_size = 60  # 1-hour bins\n",
    "\n",
    "# Create the bins using numpy\n",
    "bins = np.arange(0, 24*60 + bin_size, bin_size)\n",
    "\n",
    "# Plot the histogram using seaborn's histplot with time bins\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=distance_walking_running, x='time_minutes', bins=bins, kde=True)\n",
    "plt.xticks(range(0, 24*60+1, bin_size), [f'{i:02d}:00' for i in range(0, 24+1, bin_size//60)], rotation=45)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_walking_running['time'].dt.seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data into one DataFrame\n",
    "\n",
    "for d in [basal_energy_burned, body_mass, flights_climbed, step_count, body_mass_index, distance_walking_running]:\n",
    "    df = df.merge(d, on=\"date\", how=\"left\")\n",
    "\n",
    "# Time series data so use ffill\n",
    "df = df.fillna(method=\"ffill\")\n",
    "# Note: also ffills sleep_hours \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactions\n",
    "df[\"distance_per_step\"] = df[\"distance\"] / df[\"step_count\"] # To account for jumping, hiking, etc.\n",
    "df[\"calorie_per_step\"] = df[\"basal_energy_burned\"] / df[\"step_count\"] # To account for intensity of exercise\n",
    "d[\"calorie_per_distance\"] = df[\"basal_energy_burned\"] / df[\"distance\"] # Gym days vs. Outdoor days\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series predictions to come later\n",
    "# Try a column for every sleep_hours for the past week"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# YData Profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 19:54:54,175 - INFO - Reading train data from ./data/train.csv\n",
      "2023-03-22 19:54:54,180 - INFO - Start date: 2015-02-19\n",
      "2023-03-22 19:54:54,180 - INFO - End date: 2021-12-31\n",
      "2023-03-22 19:54:54,183 - INFO - missing days: 154\n",
      "2023-03-22 19:54:54,183 - INFO - Featurizing time series data\n",
      "2023-03-22 19:54:54,218 - INFO - Featurizing XML data\n",
      "2023-03-22 19:54:59,441 - INFO - Imputing missing values...\n",
      "2023-03-22 19:54:59,444 - INFO - Creating interactions...\n",
      "2023-03-22 19:54:59,448 - INFO - Saving to Parquet file...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'sleep_hours', 'day_of_year', 'doy_sin', 'doy_cos', 'BasalEnergyBurned', 'BodyMass', 'BodyMass_hours_between', 'FlightsClimbed', 'FlightsClimbed_hours_between', 'StepCount', 'StepCount_hours_between', 'BodyMassIndex', 'DistanceWalkingRunning', 'DistanceWalkingRunning_hours_between', 'distance_per_step', 'calorie_per_step', 'calorie_per_distance']\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "\n",
    "df, f_transformer, t_transformer, cols = create_data(\"./data/train.csv\", type=\"train\")\n",
    "\n",
    "\n",
    "print(df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2acbc4615674c75a1c76188fd9038bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb7c2ea11084147ae9ff97306a464ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3683a51d12e49a5a7fc19ff18331e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba067515cf7450886980a9dc9f5f21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d89e817bce347e88e43ec96ee06b367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5768adf26e4f6d827492aca28859f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ed4c9ff82a46098bde061371d8fa14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518b347e126849dc9f2722e2e50170a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile_main = ProfileReport(df, title=\"POG4 Profiling Report\", explorative=True)\n",
    "profile_main.to_file(\"data_profile_main.html\")\n",
    "\n",
    "profile_ts = ProfileReport(df, tsmode=True, sortby=\"date\", title=\"Time-Series EDA\", explorative=True)\n",
    "profile_ts.to_file(\"data_profile_ts.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sleep_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>6.785190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>6.697302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>6.634091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>6.548654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>6.512289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  sleep_hours\n",
       "0  2022-01-01     6.785190\n",
       "1  2022-01-02     6.697302\n",
       "2  2022-01-03     6.634091\n",
       "3  2022-01-04     6.548654\n",
       "4  2022-01-05     6.512289"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge Submissions test\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "files = glob(\"./submissions/*.csv\")\n",
    "\n",
    "sub_all = pd.concat([pd.read_csv(f) for f in files], axis=0, ignore_index=True)\n",
    "sub_all = sub_all.groupby(\"date\")[\"sleep_hours\"].mean().reset_index()\n",
    "sub_all.to_csv(\"./submissions/submission_combined.csv\", index=False)\n",
    "sub_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pog4-sleep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
