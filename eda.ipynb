{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "# Make sure sorted by date\n",
    "df = df.sort_values(by='date')\n",
    "sub = sub.sort_values(by='date')\n",
    "\n",
    "# Format date as date\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
    "sub[\"date\"] = pd.to_datetime(sub[\"date\"]).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is going to be a common problem to fix, but tracking is doubled 9/27/2017 and 6/12/2018\n",
    "\n",
    "def fix_double_tracking(df, col, date_col=\"date\"):\n",
    "    df.loc[(df[date_col] <= pd.to_datetime(\"2018-06-12\").date()) & (df[date_col] >= pd.to_datetime(\"2017-09-27\").date()), col] = \\\n",
    "    df.loc[(df[date_col] <= pd.to_datetime(\"2018-06-12\").date()) & (df[date_col] >= pd.to_datetime(\"2017-09-27\").date()), col] / 2\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleep_hours between 9/27/2017 and 6/12/2018 is doubled and needs to be divided by 2\n",
    "df = fix_double_tracking(df, \"sleep_hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sleep_hours.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing dates (for time series)\n",
    "\n",
    "start_date = df[\"date\"].min()\n",
    "print(f\"Start date: {start_date}\")\n",
    "\n",
    "end_date = df[\"date\"].max()\n",
    "print(f\"End date: {end_date}\")\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "date_range = pd.DataFrame({\"date\": date_range})\n",
    "date_range[\"date\"] = date_range[\"date\"].dt.date\n",
    "\n",
    "df = date_range.merge(df, on=\"date\", how=\"left\")\n",
    "print('missing days: ', df.sleep_hours.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward fill the missing values/targets\n",
    "df[\"sleep_hours\"] = df[\"sleep_hours\"].fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot df_sleep.sleep_hours as dots by df_sleep.date\n",
    "df.plot.scatter(x='date', y='sleep_hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median of sleep_hours\n",
    "med = df.sleep_hours.median()\n",
    "print(f\"Median sleep hours: {med}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_median = sub.copy()\n",
    "sub_median[\"sleep_hours\"] = med\n",
    "sub_median.to_csv(\"./submissions/submission_median.csv\", index=False)\n",
    "sub_median.head()\n",
    "\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_median.csv -m \"Trying just all median\"\n",
    "# Score : 0.67685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median of sleep_hours\n",
    "mean = df.sleep_hours.mean()\n",
    "print(f\"Median sleep hours: {mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_mean = sub.copy()\n",
    "sub_mean[\"sleep_hours\"] = mean\n",
    "sub_mean.to_csv(\"./submissions/submission_mean.csv\", index=False)\n",
    "sub_mean.head()\n",
    "\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_mean.csv -m \"Trying just all mean\"\n",
    "# Score: 0.67396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "### Time series Data ###\n",
    "\n",
    "df[\"day_of_week\"] = pd.to_datetime(df[\"date\"]).dt.dayofweek # Day of the week\n",
    "df[\"month\"] = pd.to_datetime(df[\"date\"]).dt.month # Month\n",
    "df[\"is_weekend\"] = df[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0) # is_weekend\n",
    "\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start=\"2014-01-01\", end=\"2023-12-31\") \n",
    "df[\"is_holiday\"] = df[\"date\"].apply(lambda x: 1 if x in holidays else 0) # is_holiday\n",
    "df[\"is_workday\"] = 1-(df[\"is_weekend\"] + df[\"is_holiday\"]) # is_workday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submissions by median by DOW\n",
    "sub_median_group = sub.copy()\n",
    "med = df.groupby(\"day_of_week\")[\"sleep_hours\"].median()\n",
    "sub_median_group[\"day_of_week\"] = pd.to_datetime(sub_median_group[\"date\"]).dt.dayofweek\n",
    "sub_median_group[\"sleep_hours\"] = sub_median_group[\"day_of_week\"].map(med)\n",
    "sub_median_group = sub_median_group.drop(\"day_of_week\", axis=1)\n",
    "sub_median_group.to_csv(\"./submissions/submission_median_group.csv\", index=False)\n",
    "sub_median_group.head()\n",
    "\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_median_group.csv -m \"Trying all median by DOW\"\n",
    "# Score: 0.67406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submissions by median by DOW\n",
    "sub_mean_group = sub.copy()\n",
    "mean = df.groupby(\"day_of_week\")[\"sleep_hours\"].mean()\n",
    "sub_mean_group[\"day_of_week\"] = pd.to_datetime(sub_mean_group[\"date\"]).dt.dayofweek\n",
    "sub_mean_group[\"sleep_hours\"] = sub_mean_group[\"day_of_week\"].map(mean)\n",
    "sub_mean_group = sub_mean_group.drop(\"day_of_week\", axis=1)\n",
    "sub_mean_group.to_csv(\"./submissions/submission_mean_group.csv\", index=False)\n",
    "sub_mean_group.head()\n",
    "\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_mean_group.csv -m \"Trying all mean by DOW\"\n",
    "# Score: ________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission by median by DOW and and is_workday\n",
    "sub_median_group_workday = sub.copy()\n",
    "med = df.groupby([\"day_of_week\", \"is_workday\"])[\"sleep_hours\"].median()\n",
    "sub_median_group_workday[\"day_of_week\"] = pd.to_datetime(sub_median_group_workday[\"date\"]).dt.dayofweek\n",
    "sub_median_group_workday[\"is_workday\"] = 1-(sub_median_group_workday[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0) + sub_median_group_workday[\"date\"].apply(lambda x: 1 if x in holidays else 0))\n",
    "sub_median_group_workday[\"sleep_hours\"] = sub_median_group_workday[[\"day_of_week\", \"is_workday\"]].apply(lambda x: med[x[0]][x[1]], axis=1)\n",
    "sub_median_group_workday = sub_median_group_workday.drop([\"day_of_week\", \"is_workday\"], axis=1)\n",
    "sub_median_group_workday.to_csv(\"./submissions/submission_median_group_workday.csv\", index=False)\n",
    "sub_median_group_workday.head()\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_median_group_workday.csv -m \"Trying all median by DOW and is_workday\"\n",
    "# Score: __________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission by mean by DOW and and is_workday\n",
    "sub_mean_group_workday = sub.copy()\n",
    "mean = df.groupby([\"day_of_week\", \"is_workday\"])[\"sleep_hours\"].mean()\n",
    "sub_mean_group_workday[\"day_of_week\"] = pd.to_datetime(sub_mean_group_workday[\"date\"]).dt.dayofweek\n",
    "sub_mean_group_workday[\"is_workday\"] = 1-(sub_mean_group_workday[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0) + sub_mean_group_workday[\"date\"].apply(lambda x: 1 if x in holidays else 0))\n",
    "sub_mean_group_workday[\"sleep_hours\"] = sub_mean_group_workday[[\"day_of_week\", \"is_workday\"]].apply(lambda x: mean[x[0]][x[1]], axis=1)\n",
    "sub_mean_group_workday = sub_mean_group_workday.drop([\"day_of_week\", \"is_workday\"], axis=1)\n",
    "sub_mean_group_workday.to_csv(\"./submissions/submission_mean_group_workday.csv\", index=False)\n",
    "sub_mean_group_workday.head()\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_mean_group_workday.csv -m \"Trying all mean by DOW and is_workday\"\n",
    "# Score: ________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "csv_files = glob(\"./data/xml_export/*.csv\")\n",
    "\n",
    "# Keep what files have ~80% of the dates in the training data\n",
    "unique_dates = set(df[\"date\"])\n",
    "\n",
    "match_threshold = 0.75 # Threshold for matching\n",
    "\n",
    "def intersection_ratio(set1, set2):\n",
    "    return len(set1 & set2) / len(set1)\n",
    "\n",
    "matching_csvs = []\n",
    "for csv_file in csv_files:\n",
    "    csv_df = pd.read_csv(csv_file)  \n",
    "    try: \n",
    "        csv_df[\"date\"] = pd.to_datetime(csv_df[\"startDate\"]).dt.date\n",
    "    except:\n",
    "        csv_df[\"date\"] = pd.to_datetime(csv_df[\"dateComponents\"]).dt.date # Some use dateComponents instead of startDate\n",
    "\n",
    "    unique_creation_dates = set(csv_df)\n",
    "\n",
    "    ratio = intersection_ratio(unique_dates, unique_creation_dates)\n",
    "\n",
    "    if ratio >= match_threshold:\n",
    "        matching_csvs.append(csv_file)\n",
    "\n",
    "print(\"CSV files with at least 75% of the 'dates' from the original DataFrame:\")\n",
    "for matching_csv in matching_csvs:\n",
    "    print(matching_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_df.loc[csv_df[\"date\"] >= pd.to_datetime(\"2021-01-01\").date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [\"test1\", \"test2\", \"test3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets worth exploring\n",
    "basal_energy_burned = pd.read_csv('./data/xml_export/BasalEnergyBurned.csv', low_memory=False)\n",
    "body_mass = pd.read_csv('./data/xml_export/BodyMass.csv', low_memory=False)\n",
    "flights_climbed = pd.read_csv('./data/xml_export/FlightsClimbed.csv', low_memory=False)\n",
    "step_count = pd.read_csv('./data/xml_export/StepCount.csv', low_memory=False)\n",
    "body_mass_index = pd.read_csv('./data/xml_export/BodyMassIndex.csv', low_memory=False)\n",
    "distance_walking_running = pd.read_csv('./data/xml_export/DistanceWalkingRunning.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_xml_output(path):\n",
    "    # Import the csv\n",
    "    csv_df = pd.read_csv(path, low_memory=False)\n",
    "    base_name = os.path.basename(path).split(\".\")[0]\n",
    "    \n",
    "    # BodyMassIndex we want to avg not sum\n",
    "    if base_name == \"BodyMassIndex\":\n",
    "        agg_func = \"mean\"\n",
    "    else:\n",
    "        agg_func = \"sum\"\n",
    "    \n",
    "    # Convert startDate and endDate columns to datetime objects\n",
    "    csv_df[\"startDate\"] = pd.to_datetime(csv_df[\"startDate\"]).dt.tz_convert (\"US/Eastern\")\n",
    "    csv_df[\"endDate\"] = pd.to_datetime(csv_df[\"endDate\"]).dt.tz_convert (\"US/Eastern\")\n",
    "    # Create date & time column\n",
    "    csv_df[\"date\"] = pd.to_datetime(csv_df[\"startDate\"]).dt.date\n",
    "    csv_df[\"time\"] = pd.to_datetime(csv_df[\"startDate\"]).dt.time\n",
    "    \n",
    "    # Group by date and perform aggregations\n",
    "    csv_df = csv_df.groupby(\"date\").agg(\n",
    "        max_start_time=pd.NamedAgg(column=\"startDate\", aggfunc=\"max\"),\n",
    "        min_start_time=pd.NamedAgg(column=\"startDate\", aggfunc=\"min\"),\n",
    "        max_end_time=pd.NamedAgg(column=\"endDate\", aggfunc=\"max\"),\n",
    "        min_end_time=pd.NamedAgg(column=\"endDate\", aggfunc=\"min\"),\n",
    "        value_sum=pd.NamedAgg(column=\"value\", aggfunc=agg_func)\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Check if the dates match between min_start_time and max_end_time\n",
    "    csv_df[\"dates_match\"] = csv_df.apply(\n",
    "        lambda row: row[\"min_start_time\"].date() == row[\"max_end_time\"].date(), axis=1)\n",
    "    csv_df = csv_df.rename(columns={\"value_sum\": base_name})\n",
    "    \n",
    "    # Assert if dates match in all rows\n",
    "    # assert csv_df[\"dates_match\"].all(), \"Dates do not match in some rows\"\n",
    "    \n",
    "    # Trigonomic Hours\n",
    "    csv_df[base_name+\"_mx_st_hr_sin\"] = np.sin(2 * np.pi * csv_df[\"max_start_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mx_st_hr_cos\"] = np.cos(2 * np.pi * csv_df[\"max_start_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mn_st_hr_sin\"] = np.sin(2 * np.pi * csv_df[\"min_start_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mn_st_hr_cos\"] = np.cos(2 * np.pi * csv_df[\"min_start_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mx_et_hr_sin\"] = np.sin(2 * np.pi * csv_df[\"max_end_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mx_et_hr_cos\"] = np.cos(2 * np.pi * csv_df[\"max_end_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mn_et_hr_sin\"] = np.sin(2 * np.pi * csv_df[\"min_end_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mn_et_hr_cos\"] = np.cos(2 * np.pi * csv_df[\"min_end_time\"].dt.hour / 24) \n",
    "    \n",
    "    #! HIGHLY ILLEGAL FEATURE??? (Step Count basically calculates sleep time)\n",
    "    # Hours between startDate and next startDate (lag = -1)\n",
    "    csv_df[base_name+\"_hours_between\"] = (csv_df[\"min_start_time\"].shift(-1) - csv_df[\"max_start_time\"]).dt.total_seconds() / 3600\n",
    "    \n",
    "    #Fix double tracking\n",
    "    csv_df = fix_double_tracking(csv_df, base_name)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    # csv_df = csv_df.drop(columns=[\"max_start_time\", \"min_start_time\", \"max_end_time\", \"min_end_time\", \"dates_match\"])\n",
    "\n",
    "    return csv_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basal Energy Burned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basal_energy_burned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value is the only useful feature\n",
    "# Group by date and sum (set alias to basal_energy_burned)\n",
    "\n",
    "# NOTE: Summing assumes duplicate tracking is not occuring between devices\n",
    "\n",
    "basal_energy_burned = parse_xml_output(\"./data/xml_export/BasalEnergyBurned.csv\")\n",
    "basal_energy_burned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "basal_energy_burned.BasalEnergyBurned.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basal_energy_burned.plot.scatter(x='date', y='BasalEnergyBurned')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Rob got a better tracker or became more serious about racking in 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass[\"date\"] = pd.to_datetime(body_mass[\"startDate\"]).dt.date\n",
    "body_mass = body_mass.groupby(\"date\").sum().reset_index()\n",
    "body_mass = body_mass.rename(columns={\"value\": \"body_mass\"})\n",
    "body_mass = fix_double_tracking(body_mass, \"body_mass\")\n",
    "body_mass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "body_mass.body_mass.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass.plot.scatter(x='date', y='body_mass')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flights Climbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_climbed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_climbed[\"date\"] = pd.to_datetime(flights_climbed[\"startDate\"]).dt.date\n",
    "flights_climbed = flights_climbed.groupby(\"date\").sum().reset_index()\n",
    "flights_climbed = flights_climbed.rename(columns={\"value\": \"flights_climbed\"})\n",
    "flights_climbed = fix_double_tracking(flights_climbed, \"flights_climbed\")\n",
    "flights_climbed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "flights_climbed.flights_climbed.hist()\n",
    "\n",
    "# Definitely lognormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_climbed.plot.scatter(x='date', y='flights_climbed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very weird data. Will have to explore further. Possible duplication?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count[\"date\"] = pd.to_datetime(step_count[\"startDate\"]).dt.date\n",
    "step_count = step_count.groupby(\"date\").sum().reset_index()\n",
    "step_count = step_count.rename(columns={\"value\": \"step_count\"})\n",
    "step_count = fix_double_tracking(step_count, \"step_count\")\n",
    "step_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "step_count.step_count.hist()\n",
    "\n",
    "#kind of lognormal but not quite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count.plot.scatter(x='date', y='step_count')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body Mass Index\n",
    "This is a slow changing variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass_index[\"date\"] = pd.to_datetime(body_mass_index[\"startDate\"]).dt.date\n",
    "body_mass_index = body_mass_index.groupby(\"date\").mean(numeric_only =True).reset_index() #! MEAN here\n",
    "body_mass_index = body_mass_index.rename(columns={\"value\": \"body_mass_index\"})\n",
    "body_mass_index = body_mass_index.drop(columns=[\"sourceVersion\", \"device\"], axis=1)\n",
    "body_mass_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "body_mass_index.body_mass_index.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass_index.plot.scatter(x='date', y='body_mass_index')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not measured frequently in 2012-2015"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Walking/Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_walking_running.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_walking_running = parse_xml_output(\"./data/xml_export/DistanceWalkingRunning.csv\")\n",
    "distance_walking_running.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "distance_walking_running.DistanceWalkingRunning.hist()\n",
    "\n",
    "# Lognormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_walking_running.plot.scatter(x='date', y='DistanceWalkingRunning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the hour and minute information from the datetime column and convert it to minutes\n",
    "distance_walking_running['time_minutes'] = distance_walking_running['min_start_time'].dt.hour * 60 + distance_walking_running['min_start_time'].dt.minute\n",
    "\n",
    "# Define the bin size (in minutes)\n",
    "bin_size = 60  # 1-hour bins\n",
    "\n",
    "# Create the bins using numpy\n",
    "bins = np.arange(0, 24*60 + bin_size, bin_size)\n",
    "\n",
    "# Plot the histogram using seaborn's histplot with time bins\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=distance_walking_running, x='time_minutes', bins=bins, kde=True)\n",
    "plt.xticks(range(0, 24*60+1, bin_size), [f'{i:02d}:00' for i in range(0, 24+1, bin_size//60)], rotation=45)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_walking_running['time'].dt.seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data into one DataFrame\n",
    "\n",
    "for d in [basal_energy_burned, body_mass, flights_climbed, step_count, body_mass_index, distance_walking_running]:\n",
    "    df = df.merge(d, on=\"date\", how=\"left\")\n",
    "\n",
    "# Time series data so use ffill\n",
    "df = df.fillna(method=\"ffill\")\n",
    "# Note: also ffills sleep_hours \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactions\n",
    "df[\"distance_per_step\"] = df[\"distance\"] / df[\"step_count\"] # To account for jumping, hiking, etc.\n",
    "df[\"calorie_per_step\"] = df[\"basal_energy_burned\"] / df[\"step_count\"] # To account for intensity of exercise\n",
    "d[\"calorie_per_distance\"] = df[\"basal_energy_burned\"] / df[\"distance\"] # Gym days vs. Outdoor days\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series predictions to come later\n",
    "# Try a column for every sleep_hours for the past week"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# YData Profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 01:20:38,115 - INFO - Creating XML data\n",
      "2023-03-23 01:20:43,160 - INFO - missing days: 154\n",
      "2023-03-23 01:20:43,162 - INFO - Featurizing time series data\n",
      "2023-03-23 01:20:43,193 - INFO - Creating interactions...\n",
      "2023-03-23 01:20:43,206 - INFO - dropped non-unique columns: ['BasalEnergyBurned_startDate_min_hr_sin', 'BasalEnergyBurned_startDate_min_hr_cos', 'StepCount_startDate_min_hr_sin', 'StepCount_startDate_min_hr_cos', 'BodyMassIndex_startDate_max_hr_sin', 'BodyMassIndex_startDate_max_hr_cos', 'BodyMassIndex_startDate_min_hr_sin', 'BodyMassIndex_startDate_min_hr_cos', 'BodyMassIndex_endDate_max_hr_sin', 'BodyMassIndex_endDate_max_hr_cos', 'BodyMassIndex_endDate_min_hr_sin', 'BodyMassIndex_endDate_min_hr_cos', 'BodyMassIndex_hrs_btween', 'DistanceWalkingRunning_startDate_min_hr_sin', 'DistanceWalkingRunning_startDate_min_hr_cos', 'is_holiday']\n",
      "2023-03-23 01:20:43,210 - INFO - dropped null columns: []\n",
      "2023-03-23 01:20:43,213 - INFO - Featurizing time series data\n",
      "2023-03-23 01:20:43,226 - INFO - Creating interactions...\n"
     ]
    }
   ],
   "source": [
    "from helper import POG4_Dataset\n",
    "\n",
    "data = POG4_Dataset()\n",
    "\n",
    "#print(df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'sleep_hours', 'BasalEnergyBurned',\n",
       "       'BasalEnergyBurned_startDate_max_hr_sin',\n",
       "       'BasalEnergyBurned_startDate_max_hr_cos',\n",
       "       'BasalEnergyBurned_endDate_max_hr_sin',\n",
       "       'BasalEnergyBurned_endDate_max_hr_cos',\n",
       "       'BasalEnergyBurned_endDate_min_hr_sin',\n",
       "       'BasalEnergyBurned_endDate_min_hr_cos', 'BasalEnergyBurned_hrs_btween',\n",
       "       'BodyMass', 'BodyMass_startDate_max_hr_sin',\n",
       "       'BodyMass_startDate_max_hr_cos', 'BodyMass_startDate_min_hr_sin',\n",
       "       'BodyMass_startDate_min_hr_cos', 'BodyMass_endDate_max_hr_sin',\n",
       "       'BodyMass_endDate_max_hr_cos', 'BodyMass_endDate_min_hr_sin',\n",
       "       'BodyMass_endDate_min_hr_cos', 'BodyMass_hrs_btween', 'FlightsClimbed',\n",
       "       'FlightsClimbed_startDate_max_hr_sin',\n",
       "       'FlightsClimbed_startDate_max_hr_cos',\n",
       "       'FlightsClimbed_startDate_min_hr_sin',\n",
       "       'FlightsClimbed_startDate_min_hr_cos',\n",
       "       'FlightsClimbed_endDate_max_hr_sin',\n",
       "       'FlightsClimbed_endDate_max_hr_cos',\n",
       "       'FlightsClimbed_endDate_min_hr_sin',\n",
       "       'FlightsClimbed_endDate_min_hr_cos', 'FlightsClimbed_hrs_btween',\n",
       "       'StepCount', 'StepCount_startDate_max_hr_sin',\n",
       "       'StepCount_startDate_max_hr_cos', 'StepCount_endDate_max_hr_sin',\n",
       "       'StepCount_endDate_max_hr_cos', 'StepCount_endDate_min_hr_sin',\n",
       "       'StepCount_endDate_min_hr_cos', 'StepCount_hrs_btween', 'BodyMassIndex',\n",
       "       'DistanceWalkingRunning', 'DistanceWalkingRunning_startDate_max_hr_sin',\n",
       "       'DistanceWalkingRunning_startDate_max_hr_cos',\n",
       "       'DistanceWalkingRunning_endDate_max_hr_sin',\n",
       "       'DistanceWalkingRunning_endDate_max_hr_cos',\n",
       "       'DistanceWalkingRunning_endDate_min_hr_sin',\n",
       "       'DistanceWalkingRunning_endDate_min_hr_cos',\n",
       "       'DistanceWalkingRunning_hrs_btween', 'day_of_week', 'dow_sin',\n",
       "       'dow_cos', 'day_of_year', 'doy_sin', 'doy_cos', 'month', 'month_sin',\n",
       "       'month_cos', 'is_weekend', 'is_workday', 'is_daylight_savings',\n",
       "       'dow_median', 'distance_per_step', 'calorie_per_step',\n",
       "       'calorie_per_distance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"sleep_h\"].value_counts(normalize=True, dropna=False).iloc[0] > threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2acbc4615674c75a1c76188fd9038bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb7c2ea11084147ae9ff97306a464ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3683a51d12e49a5a7fc19ff18331e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba067515cf7450886980a9dc9f5f21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d89e817bce347e88e43ec96ee06b367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n",
      "/home/sebastian/mambaforge/envs/pog4-sleep/lib/python3.9/site-packages/ydata_profiling/model/pandas/describe_timeseries_pandas.py:117: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  mad = pos_fft_above_med[\"ampl\"].mad()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5768adf26e4f6d827492aca28859f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ed4c9ff82a46098bde061371d8fa14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518b347e126849dc9f2722e2e50170a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile_main = ProfileReport(df, title=\"POG4 Profiling Report\", explorative=True)\n",
    "profile_main.to_file(\"data_profile_main.html\")\n",
    "\n",
    "profile_ts = ProfileReport(df, tsmode=True, sortby=\"date\", title=\"Time-Series EDA\", explorative=True)\n",
    "profile_ts.to_file(\"data_profile_ts.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sleep_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>6.785190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>6.697302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>6.634091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>6.548654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>6.512289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  sleep_hours\n",
       "0  2022-01-01     6.785190\n",
       "1  2022-01-02     6.697302\n",
       "2  2022-01-03     6.634091\n",
       "3  2022-01-04     6.548654\n",
       "4  2022-01-05     6.512289"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge Submissions test\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "files = glob(\"./submissions/*.csv\")\n",
    "\n",
    "sub_all = pd.concat([pd.read_csv(f) for f in files], axis=0, ignore_index=True)\n",
    "sub_all = sub_all.groupby(\"date\")[\"sleep_hours\"].mean().reset_index()\n",
    "sub_all.to_csv(\"./submissions/submission_combined.csv\", index=False)\n",
    "sub_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 16:28:24,334 - INFO - Creating XML data\n",
      "2023-03-23 16:28:29,343 - INFO - Missing days: 154\n",
      "2023-03-23 16:28:29,346 - INFO - Featurizing time series data\n",
      "2023-03-23 16:28:29,377 - INFO - Creating interactions...\n",
      "2023-03-23 16:28:29,390 - INFO - Dropped non-unique columns: ['BasalEnergyBurned_startDate_min_hr_sin', 'BasalEnergyBurned_startDate_min_hr_cos', 'StepCount_startDate_min_hr_sin', 'StepCount_startDate_min_hr_cos', 'BodyMassIndex_startDate_max_hr_sin', 'BodyMassIndex_startDate_max_hr_cos', 'BodyMassIndex_startDate_min_hr_sin', 'BodyMassIndex_startDate_min_hr_cos', 'BodyMassIndex_endDate_max_hr_sin', 'BodyMassIndex_endDate_max_hr_cos', 'BodyMassIndex_endDate_min_hr_sin', 'BodyMassIndex_endDate_min_hr_cos', 'BodyMassIndex_hrs_btween', 'DistanceWalkingRunning_startDate_min_hr_sin', 'DistanceWalkingRunning_startDate_min_hr_cos', 'is_holiday']\n",
      "2023-03-23 16:28:29,396 - INFO - Dropped null columns: []\n",
      "2023-03-23 16:28:29,398 - INFO - Splitting data into train and test set\n",
      "2023-03-23 16:28:29,399 - INFO - Scaling and imputing data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(      BasalEnergyBurned  BasalEnergyBurned_startDate_max_hr_sin  \\\n",
       " 0             -0.337965                                     0.0   \n",
       " 1             -0.337965                                     0.0   \n",
       " 2             -0.337965                                     0.0   \n",
       " 3             -0.337965                                     0.0   \n",
       " 4             -0.337965                                     0.0   \n",
       " ...                 ...                                     ...   \n",
       " 2001           0.244545                                     0.0   \n",
       " 2002           0.232657                                     0.0   \n",
       " 2003           0.185105                                     0.0   \n",
       " 2004           0.220769                                     0.0   \n",
       " 2005           0.232657                                     0.0   \n",
       " \n",
       "       BasalEnergyBurned_startDate_max_hr_cos  \\\n",
       " 0                                        0.0   \n",
       " 1                                        0.0   \n",
       " 2                                        0.0   \n",
       " 3                                        0.0   \n",
       " 4                                        0.0   \n",
       " ...                                      ...   \n",
       " 2001                                     0.0   \n",
       " 2002                                     0.0   \n",
       " 2003                                     0.0   \n",
       " 2004                                     0.0   \n",
       " 2005                                     0.0   \n",
       " \n",
       "       BasalEnergyBurned_endDate_max_hr_sin  \\\n",
       " 0                                      0.0   \n",
       " 1                                      0.0   \n",
       " 2                                      0.0   \n",
       " 3                                      0.0   \n",
       " 4                                      0.0   \n",
       " ...                                    ...   \n",
       " 2001                                   0.0   \n",
       " 2002                                   0.0   \n",
       " 2003                                   0.0   \n",
       " 2004                                   0.0   \n",
       " 2005                                   0.0   \n",
       " \n",
       "       BasalEnergyBurned_endDate_max_hr_cos  \\\n",
       " 0                             3.330669e-16   \n",
       " 1                             3.330669e-16   \n",
       " 2                             3.330669e-16   \n",
       " 3                             3.330669e-16   \n",
       " 4                             3.330669e-16   \n",
       " ...                                    ...   \n",
       " 2001                          3.330669e-16   \n",
       " 2002                          3.330669e-16   \n",
       " 2003                          3.330669e-16   \n",
       " 2004                          3.330669e-16   \n",
       " 2005                          3.330669e-16   \n",
       " \n",
       "       BasalEnergyBurned_endDate_min_hr_sin  \\\n",
       " 0                                      0.0   \n",
       " 1                                      0.0   \n",
       " 2                                      0.0   \n",
       " 3                                      0.0   \n",
       " 4                                      0.0   \n",
       " ...                                    ...   \n",
       " 2001                                   0.0   \n",
       " 2002                                   0.0   \n",
       " 2003                                   0.0   \n",
       " 2004                                   0.0   \n",
       " 2005                                   0.0   \n",
       " \n",
       "       BasalEnergyBurned_endDate_min_hr_cos  BasalEnergyBurned_hrs_btween  \\\n",
       " 0                             3.330669e-16                      0.006732   \n",
       " 1                             3.330669e-16                      0.006732   \n",
       " 2                             3.330669e-16                      0.006732   \n",
       " 3                             3.330669e-16                      0.006732   \n",
       " 4                             3.330669e-16                      0.006732   \n",
       " ...                                    ...                           ...   \n",
       " 2001                          3.330669e-16                      0.006732   \n",
       " 2002                          3.330669e-16                      0.006732   \n",
       " 2003                          3.330669e-16                      0.006732   \n",
       " 2004                          3.330669e-16                      0.006732   \n",
       " 2005                          3.330669e-16                      0.006732   \n",
       " \n",
       "       BodyMass  BodyMass_startDate_max_hr_sin  ...     month  month_sin  \\\n",
       " 0    -0.721941                       0.022333  ... -1.309042   1.197048   \n",
       " 1    -0.721941                       0.022333  ... -1.309042   1.197048   \n",
       " 2    -0.721941                       0.022333  ... -1.309042   1.197048   \n",
       " 3    -0.721941                       0.022333  ... -1.309042   1.197048   \n",
       " 4    -0.721941                       0.022333  ... -1.309042   1.197048   \n",
       " ...        ...                            ...  ...       ...        ...   \n",
       " 2001  0.730752                       0.022333  ...  0.478324  -1.258819   \n",
       " 2002  0.673711                       0.022333  ...  0.478324  -1.258819   \n",
       " 2003  0.616413                       0.022333  ...  0.478324  -1.258819   \n",
       " 2004  0.629280                       0.022333  ...  0.478324  -1.258819   \n",
       " 2005  0.642060                       0.022333  ...  0.478324  -1.258819   \n",
       " \n",
       "       month_cos  is_weekend  is_workday  is_daylight_savings  dow_median  \\\n",
       " 0      0.781634   -0.633118    0.633118            -1.436174    0.226962   \n",
       " 1      0.781634   -0.633118    0.633118            -1.436174    0.498094   \n",
       " 2      0.781634    1.579485   -1.579485            -1.436174    1.691075   \n",
       " 3      0.781634    1.579485   -1.579485            -1.436174    0.715000   \n",
       " 4      0.781634   -0.633118    0.633118            -1.436174   -1.128699   \n",
       " ...         ...         ...         ...                  ...         ...   \n",
       " 2001  -0.633460   -0.633118    0.633118             0.696295   -0.884680   \n",
       " 2002  -0.633460   -0.633118    0.633118             0.696295    0.226962   \n",
       " 2003  -0.633460   -0.633118    0.633118             0.696295    0.498094   \n",
       " 2004  -0.633460    1.579485   -1.579485             0.696295    1.691075   \n",
       " 2005  -0.633460    1.579485   -1.579485             0.696295    0.715000   \n",
       " \n",
       "       distance_per_step  calorie_per_step  calorie_per_distance  \n",
       " 0             -0.324166          4.837700              4.451521  \n",
       " 1             -0.317748         -0.398820             -0.342774  \n",
       " 2             -0.320014          0.628065              0.596452  \n",
       " 3             -0.324212         -0.548564             -0.477495  \n",
       " 4             -0.324169          1.241915              1.160980  \n",
       " ...                 ...               ...                   ...  \n",
       " 2001          -0.398251          0.540870              0.568441  \n",
       " 2002          -0.386252          0.102198              0.150425  \n",
       " 2003           0.414197         -0.873956             -0.901927  \n",
       " 2004          -0.403319         -0.719320             -0.612563  \n",
       " 2005          -0.453395         -0.165476             -0.068913  \n",
       " \n",
       " [2006 rows x 61 columns],\n",
       "      BasalEnergyBurned  BasalEnergyBurned_startDate_max_hr_sin  \\\n",
       " 0             0.244545                                0.000000   \n",
       " 1             0.244545                                0.000000   \n",
       " 2             0.244545                                0.000000   \n",
       " 3             0.244545                                0.000000   \n",
       " 4             0.244545                                0.000000   \n",
       " ..                 ...                                     ...   \n",
       " 497           2.804021                               -0.258819   \n",
       " 498         -17.358165                                0.965926   \n",
       " 499         -19.966703                               -0.258819   \n",
       " 500           1.673025                               -0.258819   \n",
       " 501           0.736171                               -0.258819   \n",
       " \n",
       "      BasalEnergyBurned_startDate_max_hr_cos  \\\n",
       " 0                                  0.000000   \n",
       " 1                                  0.000000   \n",
       " 2                                  0.000000   \n",
       " 3                                  0.000000   \n",
       " 4                                  0.000000   \n",
       " ..                                      ...   \n",
       " 497                               -0.034074   \n",
       " 498                               -0.741181   \n",
       " 499                               -0.034074   \n",
       " 500                               -0.034074   \n",
       " 501                               -0.034074   \n",
       " \n",
       "      BasalEnergyBurned_endDate_max_hr_sin  \\\n",
       " 0                                0.000000   \n",
       " 1                                0.000000   \n",
       " 2                                0.000000   \n",
       " 3                                0.000000   \n",
       " 4                                0.000000   \n",
       " ..                                    ...   \n",
       " 497                              0.258819   \n",
       " 498                              1.224745   \n",
       " 499                              0.258819   \n",
       " 500                              0.258819   \n",
       " 501                              0.258819   \n",
       " \n",
       "      BasalEnergyBurned_endDate_max_hr_cos  \\\n",
       " 0                            3.330669e-16   \n",
       " 1                            3.330669e-16   \n",
       " 2                            3.330669e-16   \n",
       " 3                            3.330669e-16   \n",
       " 4                            3.330669e-16   \n",
       " ..                                    ...   \n",
       " 497                          3.407417e-02   \n",
       " 498                         -7.071068e-01   \n",
       " 499                          3.407417e-02   \n",
       " 500                          3.407417e-02   \n",
       " 501                          3.407417e-02   \n",
       " \n",
       "      BasalEnergyBurned_endDate_min_hr_sin  \\\n",
       " 0                                0.000000   \n",
       " 1                                0.000000   \n",
       " 2                                0.000000   \n",
       " 3                                0.000000   \n",
       " 4                                0.000000   \n",
       " ..                                    ...   \n",
       " 497                              0.258819   \n",
       " 498                              0.258819   \n",
       " 499                             -0.448288   \n",
       " 500                              0.258819   \n",
       " 501                              0.258819   \n",
       " \n",
       "      BasalEnergyBurned_endDate_min_hr_cos  BasalEnergyBurned_hrs_btween  \\\n",
       " 0                            3.330669e-16                      0.006732   \n",
       " 1                            3.330669e-16                      0.006732   \n",
       " 2                            3.330669e-16                      0.006732   \n",
       " 3                            3.330669e-16                      0.006732   \n",
       " 4                            3.330669e-16                      0.006732   \n",
       " ..                                    ...                           ...   \n",
       " 497                          3.407417e-02                   -320.732980   \n",
       " 498                          3.407417e-02                    210.196992   \n",
       " 499                         -2.588190e-01                   -320.732980   \n",
       " 500                          3.407417e-02                   -320.736732   \n",
       " 501                          3.407417e-02                   -320.732980   \n",
       " \n",
       "      BodyMass  BodyMass_startDate_max_hr_sin  ...     month  month_sin  \\\n",
       " 0    0.654926                       0.022333  ...  0.478324  -1.258819   \n",
       " 1    0.667793                       0.022333  ...  0.478324  -1.258819   \n",
       " 2    0.680659                       0.022333  ...  0.478324  -1.258819   \n",
       " 3    0.693525                       0.022333  ...  0.478324  -1.258819   \n",
       " 4    0.706392                       0.022333  ...  0.478324  -1.258819   \n",
       " ..        ...                            ...  ...       ...        ...   \n",
       " 497 -0.044530                       0.022333  ...  1.669901  -0.030886   \n",
       " 498 -0.044530                       0.022333  ...  1.669901  -0.030886   \n",
       " 499 -0.044530                       0.022333  ...  1.669901  -0.030886   \n",
       " 500 -0.378754                     -63.333811  ...  1.669901  -0.030886   \n",
       " 501 -0.653235                     -61.175003  ...  1.669901  -0.030886   \n",
       " \n",
       "      month_cos  is_weekend  is_workday  is_daylight_savings  dow_median  \\\n",
       " 0    -0.633460   -0.633118    0.633118             0.696295   -1.128699   \n",
       " 1    -0.633460   -0.633118    0.633118             0.696295   -1.128699   \n",
       " 2    -0.633460   -0.633118    0.633118             0.696295   -0.884680   \n",
       " 3    -0.633460   -0.633118    0.633118             0.696295    0.226962   \n",
       " 4    -0.633460   -0.633118    0.633118             0.696295    0.498094   \n",
       " ..         ...         ...         ...                  ...         ...   \n",
       " 497   1.489182   -0.633118    0.633118            -1.436174   -1.128699   \n",
       " 498   1.489182   -0.633118    0.633118            -1.436174   -1.128699   \n",
       " 499   1.489182   -0.633118    0.633118            -1.436174   -0.884680   \n",
       " 500   1.489182   -0.633118    0.633118            -1.436174    0.226962   \n",
       " 501   1.489182   -0.633118    0.633118            -1.436174    0.498094   \n",
       " \n",
       "      distance_per_step  calorie_per_step  calorie_per_distance  \n",
       " 0            -0.542141         -0.869354             -0.720506  \n",
       " 1            -0.405244         -0.890615             -0.773139  \n",
       " 2            -0.508889         -0.360779             -0.232803  \n",
       " 3            -0.435839          0.612576              0.662416  \n",
       " 4            -0.698543          3.470224              3.858015  \n",
       " ..                 ...               ...                   ...  \n",
       " 497          -0.441963          1.322317              1.342853  \n",
       " 498          -0.670737         -0.153972              0.053323  \n",
       " 499          -0.654272         -1.258732             -1.091634  \n",
       " 500           2.443673         -0.369384             -0.842809  \n",
       " 501           2.444124         -0.437633             -0.875344  \n",
       " \n",
       " [502 rows x 61 columns])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper import POG4_Dataset\n",
    "\n",
    "data = POG4_Dataset()\n",
    "data.train_test_split()\n",
    "data.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y_train.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.concat([data.X_test.reset_index(drop=True), data.y_test.reset_index(drop=True)], axis=1).isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pog4-sleep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
