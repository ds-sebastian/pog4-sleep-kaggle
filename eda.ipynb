{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "# Make sure sorted by date\n",
    "df = df.sort_values(by='date')\n",
    "sub = sub.sort_values(by='date')\n",
    "\n",
    "# Format date as date\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
    "sub[\"date\"] = pd.to_datetime(sub[\"date\"]).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is going to be a common problem to fix, but tracking is doubled 9/27/2017 and 6/12/2018\n",
    "\n",
    "def fix_double_tracking(df, col, date_col=\"date\"):\n",
    "    df.loc[(df[date_col] <= pd.to_datetime(\"2018-06-12\").date()) & (df[date_col] >= pd.to_datetime(\"2017-09-27\").date()), col] = \\\n",
    "    df.loc[(df[date_col] <= pd.to_datetime(\"2018-06-12\").date()) & (df[date_col] >= pd.to_datetime(\"2017-09-27\").date()), col] / 2\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleep_hours between 9/27/2017 and 6/12/2018 is doubled and needs to be divided by 2\n",
    "df = fix_double_tracking(df, \"sleep_hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sleep_hours.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing dates (for time series)\n",
    "\n",
    "start_date = df[\"date\"].min()\n",
    "print(f\"Start date: {start_date}\")\n",
    "\n",
    "end_date = df[\"date\"].max()\n",
    "print(f\"End date: {end_date}\")\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "date_range = pd.DataFrame({\"date\": date_range})\n",
    "date_range[\"date\"] = date_range[\"date\"].dt.date\n",
    "\n",
    "df = date_range.merge(df, on=\"date\", how=\"left\")\n",
    "print('missing days: ', df.sleep_hours.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward fill the missing values/targets\n",
    "df[\"sleep_hours\"] = df[\"sleep_hours\"].fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot df_sleep.sleep_hours as dots by df_sleep.date\n",
    "df.plot.scatter(x='date', y='sleep_hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median of sleep_hours\n",
    "med = df.sleep_hours.median()\n",
    "print(f\"Median sleep hours: {med}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_median = sub.copy()\n",
    "sub_median[\"sleep_hours\"] = med\n",
    "sub_median.to_csv(\"./submissions/submission_median.csv\", index=False)\n",
    "sub_median.head()\n",
    "\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_median.csv -m \"Trying just all median\"\n",
    "# Score : 0.67685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median of sleep_hours\n",
    "mean = df.sleep_hours.mean()\n",
    "print(f\"Median sleep hours: {mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_mean = sub.copy()\n",
    "sub_mean[\"sleep_hours\"] = mean\n",
    "sub_mean.to_csv(\"./submissions/submission_mean.csv\", index=False)\n",
    "sub_mean.head()\n",
    "\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_mean.csv -m \"Trying just all mean\"\n",
    "# Score: 0.67396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "### Time series Data ###\n",
    "\n",
    "df[\"day_of_week\"] = pd.to_datetime(df[\"date\"]).dt.dayofweek # Day of the week\n",
    "df[\"month\"] = pd.to_datetime(df[\"date\"]).dt.month # Month\n",
    "df[\"is_weekend\"] = df[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0) # is_weekend\n",
    "\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start=\"2014-01-01\", end=\"2023-12-31\") \n",
    "df[\"is_holiday\"] = df[\"date\"].apply(lambda x: 1 if x in holidays else 0) # is_holiday\n",
    "df[\"is_workday\"] = 1-(df[\"is_weekend\"] + df[\"is_holiday\"]) # is_workday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submissions by median by DOW\n",
    "sub_median_group = sub.copy()\n",
    "med = df.groupby(\"day_of_week\")[\"sleep_hours\"].median()\n",
    "sub_median_group[\"day_of_week\"] = pd.to_datetime(sub_median_group[\"date\"]).dt.dayofweek\n",
    "sub_median_group[\"sleep_hours\"] = sub_median_group[\"day_of_week\"].map(med)\n",
    "sub_median_group = sub_median_group.drop(\"day_of_week\", axis=1)\n",
    "sub_median_group.to_csv(\"./submissions/submission_median_group.csv\", index=False)\n",
    "sub_median_group.head()\n",
    "\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_median_group.csv -m \"Trying all median by DOW\"\n",
    "# Score: 0.67406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submissions by median by DOW\n",
    "sub_mean_group = sub.copy()\n",
    "mean = df.groupby(\"day_of_week\")[\"sleep_hours\"].mean()\n",
    "sub_mean_group[\"day_of_week\"] = pd.to_datetime(sub_mean_group[\"date\"]).dt.dayofweek\n",
    "sub_mean_group[\"sleep_hours\"] = sub_mean_group[\"day_of_week\"].map(mean)\n",
    "sub_mean_group = sub_mean_group.drop(\"day_of_week\", axis=1)\n",
    "sub_mean_group.to_csv(\"./submissions/submission_mean_group.csv\", index=False)\n",
    "sub_mean_group.head()\n",
    "\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_mean_group.csv -m \"Trying all mean by DOW\"\n",
    "# Score: ________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission by median by DOW and and is_workday\n",
    "sub_median_group_workday = sub.copy()\n",
    "med = df.groupby([\"day_of_week\", \"is_workday\"])[\"sleep_hours\"].median()\n",
    "sub_median_group_workday[\"day_of_week\"] = pd.to_datetime(sub_median_group_workday[\"date\"]).dt.dayofweek\n",
    "sub_median_group_workday[\"is_workday\"] = 1-(sub_median_group_workday[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0) + sub_median_group_workday[\"date\"].apply(lambda x: 1 if x in holidays else 0))\n",
    "sub_median_group_workday[\"sleep_hours\"] = sub_median_group_workday[[\"day_of_week\", \"is_workday\"]].apply(lambda x: med[x[0]][x[1]], axis=1)\n",
    "sub_median_group_workday = sub_median_group_workday.drop([\"day_of_week\", \"is_workday\"], axis=1)\n",
    "sub_median_group_workday.to_csv(\"./submissions/submission_median_group_workday.csv\", index=False)\n",
    "sub_median_group_workday.head()\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_median_group_workday.csv -m \"Trying all median by DOW and is_workday\"\n",
    "# Score: __________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission by mean by DOW and and is_workday\n",
    "sub_mean_group_workday = sub.copy()\n",
    "mean = df.groupby([\"day_of_week\", \"is_workday\"])[\"sleep_hours\"].mean()\n",
    "sub_mean_group_workday[\"day_of_week\"] = pd.to_datetime(sub_mean_group_workday[\"date\"]).dt.dayofweek\n",
    "sub_mean_group_workday[\"is_workday\"] = 1-(sub_mean_group_workday[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0) + sub_mean_group_workday[\"date\"].apply(lambda x: 1 if x in holidays else 0))\n",
    "sub_mean_group_workday[\"sleep_hours\"] = sub_mean_group_workday[[\"day_of_week\", \"is_workday\"]].apply(lambda x: mean[x[0]][x[1]], axis=1)\n",
    "sub_mean_group_workday = sub_mean_group_workday.drop([\"day_of_week\", \"is_workday\"], axis=1)\n",
    "sub_mean_group_workday.to_csv(\"./submissions/submission_mean_group_workday.csv\", index=False)\n",
    "sub_mean_group_workday.head()\n",
    "# kaggle competitions submit -c kaggle-pog-series-s01e04 -f ./submissions/submission_mean_group_workday.csv -m \"Trying all mean by DOW and is_workday\"\n",
    "# Score: ________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "csv_files = glob(\"./data/xml_export/*.csv\")\n",
    "\n",
    "# Keep what files have ~80% of the dates in the training data\n",
    "unique_dates = set(df[\"date\"])\n",
    "\n",
    "match_threshold = 0.75 # Threshold for matching\n",
    "\n",
    "def intersection_ratio(set1, set2):\n",
    "    return len(set1 & set2) / len(set1)\n",
    "\n",
    "matching_csvs = []\n",
    "for csv_file in csv_files:\n",
    "    csv_df = pd.read_csv(csv_file)  \n",
    "    try: \n",
    "        csv_df[\"date\"] = pd.to_datetime(csv_df[\"startDate\"]).dt.date\n",
    "    except:\n",
    "        csv_df[\"date\"] = pd.to_datetime(csv_df[\"dateComponents\"]).dt.date # Some use dateComponents instead of startDate\n",
    "\n",
    "    unique_creation_dates = set(csv_df)\n",
    "\n",
    "    ratio = intersection_ratio(unique_dates, unique_creation_dates)\n",
    "\n",
    "    if ratio >= match_threshold:\n",
    "        matching_csvs.append(csv_file)\n",
    "\n",
    "print(\"CSV files with at least 75% of the 'dates' from the original DataFrame:\")\n",
    "for matching_csv in matching_csvs:\n",
    "    print(matching_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_df.loc[csv_df[\"date\"] >= pd.to_datetime(\"2021-01-01\").date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [\"test1\", \"test2\", \"test3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets worth exploring\n",
    "basal_energy_burned = pd.read_csv('./data/xml_export/BasalEnergyBurned.csv', low_memory=False)\n",
    "body_mass = pd.read_csv('./data/xml_export/BodyMass.csv', low_memory=False)\n",
    "flights_climbed = pd.read_csv('./data/xml_export/FlightsClimbed.csv', low_memory=False)\n",
    "step_count = pd.read_csv('./data/xml_export/StepCount.csv', low_memory=False)\n",
    "body_mass_index = pd.read_csv('./data/xml_export/BodyMassIndex.csv', low_memory=False)\n",
    "distance_walking_running = pd.read_csv('./data/xml_export/DistanceWalkingRunning.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_xml_output(path):\n",
    "    # Import the csv\n",
    "    csv_df = pd.read_csv(path, low_memory=False)\n",
    "    base_name = os.path.basename(path).split(\".\")[0]\n",
    "    \n",
    "    # BodyMassIndex we want to avg not sum\n",
    "    if base_name == \"BodyMassIndex\":\n",
    "        agg_func = \"mean\"\n",
    "    else:\n",
    "        agg_func = \"sum\"\n",
    "    \n",
    "    # Convert startDate and endDate columns to datetime objects\n",
    "    csv_df[\"startDate\"] = pd.to_datetime(csv_df[\"startDate\"]).dt.tz_convert (\"US/Eastern\")\n",
    "    csv_df[\"endDate\"] = pd.to_datetime(csv_df[\"endDate\"]).dt.tz_convert (\"US/Eastern\")\n",
    "    # Create date & time column\n",
    "    csv_df[\"date\"] = pd.to_datetime(csv_df[\"startDate\"]).dt.date\n",
    "    csv_df[\"time\"] = pd.to_datetime(csv_df[\"startDate\"]).dt.time\n",
    "    \n",
    "    # Group by date and perform aggregations\n",
    "    csv_df = csv_df.groupby(\"date\").agg(\n",
    "        max_start_time=pd.NamedAgg(column=\"startDate\", aggfunc=\"max\"),\n",
    "        min_start_time=pd.NamedAgg(column=\"startDate\", aggfunc=\"min\"),\n",
    "        max_end_time=pd.NamedAgg(column=\"endDate\", aggfunc=\"max\"),\n",
    "        min_end_time=pd.NamedAgg(column=\"endDate\", aggfunc=\"min\"),\n",
    "        value_sum=pd.NamedAgg(column=\"value\", aggfunc=agg_func)\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Check if the dates match between min_start_time and max_end_time\n",
    "    csv_df[\"dates_match\"] = csv_df.apply(\n",
    "        lambda row: row[\"min_start_time\"].date() == row[\"max_end_time\"].date(), axis=1)\n",
    "    csv_df = csv_df.rename(columns={\"value_sum\": base_name})\n",
    "    \n",
    "    # Assert if dates match in all rows\n",
    "    # assert csv_df[\"dates_match\"].all(), \"Dates do not match in some rows\"\n",
    "    \n",
    "    # Trigonomic Hours\n",
    "    csv_df[base_name+\"_mx_st_hr_sin\"] = np.sin(2 * np.pi * csv_df[\"max_start_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mx_st_hr_cos\"] = np.cos(2 * np.pi * csv_df[\"max_start_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mn_st_hr_sin\"] = np.sin(2 * np.pi * csv_df[\"min_start_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mn_st_hr_cos\"] = np.cos(2 * np.pi * csv_df[\"min_start_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mx_et_hr_sin\"] = np.sin(2 * np.pi * csv_df[\"max_end_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mx_et_hr_cos\"] = np.cos(2 * np.pi * csv_df[\"max_end_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mn_et_hr_sin\"] = np.sin(2 * np.pi * csv_df[\"min_end_time\"].dt.hour / 24)\n",
    "    csv_df[base_name+\"_mn_et_hr_cos\"] = np.cos(2 * np.pi * csv_df[\"min_end_time\"].dt.hour / 24) \n",
    "    \n",
    "    #! HIGHLY ILLEGAL FEATURE??? (Step Count basically calculates sleep time)\n",
    "    # Hours between startDate and next startDate (lag = -1)\n",
    "    csv_df[base_name+\"_hours_between\"] = (csv_df[\"min_start_time\"].shift(-1) - csv_df[\"max_start_time\"]).dt.total_seconds() / 3600\n",
    "    \n",
    "    #Fix double tracking\n",
    "    csv_df = fix_double_tracking(csv_df, base_name)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    # csv_df = csv_df.drop(columns=[\"max_start_time\", \"min_start_time\", \"max_end_time\", \"min_end_time\", \"dates_match\"])\n",
    "\n",
    "    return csv_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basal Energy Burned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basal_energy_burned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value is the only useful feature\n",
    "# Group by date and sum (set alias to basal_energy_burned)\n",
    "\n",
    "# NOTE: Summing assumes duplicate tracking is not occuring between devices\n",
    "\n",
    "basal_energy_burned = parse_xml_output(\"./data/xml_export/BasalEnergyBurned.csv\")\n",
    "basal_energy_burned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "basal_energy_burned.BasalEnergyBurned.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basal_energy_burned.plot.scatter(x='date', y='BasalEnergyBurned')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Rob got a better tracker or became more serious about racking in 2021"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass[\"date\"] = pd.to_datetime(body_mass[\"startDate\"]).dt.date\n",
    "body_mass = body_mass.groupby(\"date\").sum().reset_index()\n",
    "body_mass = body_mass.rename(columns={\"value\": \"body_mass\"})\n",
    "body_mass = fix_double_tracking(body_mass, \"body_mass\")\n",
    "body_mass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "body_mass.body_mass.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass.plot.scatter(x='date', y='body_mass')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flights Climbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_climbed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_climbed[\"date\"] = pd.to_datetime(flights_climbed[\"startDate\"]).dt.date\n",
    "flights_climbed = flights_climbed.groupby(\"date\").sum().reset_index()\n",
    "flights_climbed = flights_climbed.rename(columns={\"value\": \"flights_climbed\"})\n",
    "flights_climbed = fix_double_tracking(flights_climbed, \"flights_climbed\")\n",
    "flights_climbed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "flights_climbed.flights_climbed.hist()\n",
    "\n",
    "# Definitely lognormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_climbed.plot.scatter(x='date', y='flights_climbed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very weird data. Will have to explore further. Possible duplication?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count[\"date\"] = pd.to_datetime(step_count[\"startDate\"]).dt.date\n",
    "step_count = step_count.groupby(\"date\").sum().reset_index()\n",
    "step_count = step_count.rename(columns={\"value\": \"step_count\"})\n",
    "step_count = fix_double_tracking(step_count, \"step_count\")\n",
    "step_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "step_count.step_count.hist()\n",
    "\n",
    "#kind of lognormal but not quite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count.plot.scatter(x='date', y='step_count')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body Mass Index\n",
    "This is a slow changing variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass_index[\"date\"] = pd.to_datetime(body_mass_index[\"startDate\"]).dt.date\n",
    "body_mass_index = body_mass_index.groupby(\"date\").mean(numeric_only =True).reset_index() #! MEAN here\n",
    "body_mass_index = body_mass_index.rename(columns={\"value\": \"body_mass_index\"})\n",
    "body_mass_index = body_mass_index.drop(columns=[\"sourceVersion\", \"device\"], axis=1)\n",
    "body_mass_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "body_mass_index.body_mass_index.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass_index.plot.scatter(x='date', y='body_mass_index')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not measured frequently in 2012-2015"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Walking/Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_walking_running.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_walking_running = parse_xml_output(\"./data/xml_export/DistanceWalkingRunning.csv\")\n",
    "distance_walking_running.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "distance_walking_running.DistanceWalkingRunning.hist()\n",
    "\n",
    "# Lognormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_walking_running.plot.scatter(x='date', y='DistanceWalkingRunning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the hour and minute information from the datetime column and convert it to minutes\n",
    "distance_walking_running['time_minutes'] = distance_walking_running['min_start_time'].dt.hour * 60 + distance_walking_running['min_start_time'].dt.minute\n",
    "\n",
    "# Define the bin size (in minutes)\n",
    "bin_size = 60  # 1-hour bins\n",
    "\n",
    "# Create the bins using numpy\n",
    "bins = np.arange(0, 24*60 + bin_size, bin_size)\n",
    "\n",
    "# Plot the histogram using seaborn's histplot with time bins\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=distance_walking_running, x='time_minutes', bins=bins, kde=True)\n",
    "plt.xticks(range(0, 24*60+1, bin_size), [f'{i:02d}:00' for i in range(0, 24+1, bin_size//60)], rotation=45)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_walking_running['time'].dt.seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data into one DataFrame\n",
    "\n",
    "for d in [basal_energy_burned, body_mass, flights_climbed, step_count, body_mass_index, distance_walking_running]:\n",
    "    df = df.merge(d, on=\"date\", how=\"left\")\n",
    "\n",
    "# Time series data so use ffill\n",
    "df = df.fillna(method=\"ffill\")\n",
    "# Note: also ffills sleep_hours \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactions\n",
    "df[\"distance_per_step\"] = df[\"distance\"] / df[\"step_count\"] # To account for jumping, hiking, etc.\n",
    "df[\"calorie_per_step\"] = df[\"basal_energy_burned\"] / df[\"step_count\"] # To account for intensity of exercise\n",
    "d[\"calorie_per_distance\"] = df[\"basal_energy_burned\"] / df[\"distance\"] # Gym days vs. Outdoor days\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series predictions to come later\n",
    "# Try a column for every sleep_hours for the past week"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# YData Profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import POG4_Dataset\n",
    "\n",
    "data = POG4_Dataset()\n",
    "\n",
    "#print(df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"sleep_h\"].value_counts(normalize=True, dropna=False).iloc[0] > threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile_main = ProfileReport(df, title=\"POG4 Profiling Report\", explorative=True)\n",
    "profile_main.to_file(\"data_profile_main.html\")\n",
    "\n",
    "profile_ts = ProfileReport(df, tsmode=True, sortby=\"date\", title=\"Time-Series EDA\", explorative=True)\n",
    "profile_ts.to_file(\"data_profile_ts.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Submissions test\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "files = glob(\"./submissions/*.csv\")\n",
    "\n",
    "sub_all = pd.concat([pd.read_csv(f) for f in files], axis=0, ignore_index=True)\n",
    "sub_all = sub_all.groupby(\"date\")[\"sleep_hours\"].mean().reset_index()\n",
    "sub_all.to_csv(\"./submissions/submission_combined.csv\", index=False)\n",
    "sub_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import POG4_Dataset\n",
    "\n",
    "data = POG4_Dataset()\n",
    "#data.create_lags()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.X.DistanceWalkingRunning_nhours_avg_avg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.X.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('nas', data.X.HeartRate_sleep_hours.isna().sum())\n",
    "print(len(data.X))\n",
    "data.X.HeartRate_sleep_hours.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.X.HeartRate_sleep_hours.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.value_counts(data.X.dtypes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.concat([data.X_test.reset_index(drop=True), data.y_test.reset_index(drop=True)], axis=1).isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1915/2508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "api = wandb.Api()\n",
    "runs = api.runs(\"sgobat/pog4_xgb_classifier\")\n",
    "runs_2 = api.runs(\"sgobat/pog4_xgb\")\n",
    "# Initialize a list to store feature importances from all runs\n",
    "feature_importances_list = []\n",
    "\n",
    "# Iterate through the runs and fetch feature importances\n",
    "for run in runs:\n",
    "    # Fetch logged feature importances from the current run\n",
    "    feature_importances = {\n",
    "        key.replace(\"feature/\", \"\"): value\n",
    "        for key, value in run.summary.items()\n",
    "        if key.startswith(\"feature/\")\n",
    "    }\n",
    "    \n",
    "    # Append the fetched feature importances to the list\n",
    "    feature_importances_list.append(feature_importances)\n",
    "\n",
    "\n",
    "for run in runs_2:\n",
    "    # Fetch logged feature importances from the current run\n",
    "    feature_importances = {\n",
    "        key.replace(\"feature/\", \"\"): value\n",
    "        for key, value in run.summary.items()\n",
    "        if key.startswith(\"feature/\")\n",
    "    }\n",
    "    \n",
    "    # Append the fetched feature importances to the list\n",
    "    feature_importances_list.append(feature_importances)\n",
    "\n",
    "# Create a DataFrame from the list of feature importances\n",
    "df = pd.DataFrame(feature_importances_list)\n",
    "\n",
    "# Calculate the mean feature importances across all runs\n",
    "mean_feature_importances = df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", 1000):\n",
    "    display(mean_feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_threshold = mean_feature_importances.median() #- 1*mean_feature_importances.std()\n",
    "print(feat_imp_threshold)\n",
    "\n",
    "for i in mean_feature_importances.index:\n",
    "    if mean_feature_importances[i] > feat_imp_threshold:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get first doy_mean where is_tuesday == 1\n",
    "df.loc[df[\"is_tuesday\"] == 1, \"doy_mean\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "\"AppleStandTime_hrs_btween\",\n",
    "\"BodyMassIndex_hrs_btween\",\n",
    "\"OxygenSaturation_hrs_btween\",\n",
    "\"is_weekend\",\n",
    "\"appleExerciseTime\",\n",
    "\"AppleStandTime\",\n",
    "\"AppleStandHour_hrs_btween\",\n",
    "\"VO2Max\",\n",
    "\"AppleExerciseTime_hrs_btween\",\n",
    "\"dow_median\",\n",
    "\"appleStandHours\",\n",
    "\"AppleStandTime_night_hours\",\n",
    "\"AppleExerciseTime\",\n",
    "\"day_of_week\",\n",
    "\"avg_endDate_max_sin\",\n",
    "\"max_startDate_min_hr\",\n",
    "\"activeEnergyBurned\",\n",
    "\"avg_startDate_min_sin\",\n",
    "\"avg_startDate_min_hr\",\n",
    "\"calorie_per_step\",\n",
    "\"DistanceWalkingRunning_night_hours\",\n",
    "\"month_sin\",\n",
    "\"HeadphoneAudioExposure\",\n",
    "\"avg_startDate_min_cos\",\n",
    "\"max_endDate_min_hr\",\n",
    "\"FlightsClimbed_hrs_btween\",\n",
    "\"avg_endDate_min_sin\",\n",
    "\"avg_endDate_max_cos\",\n",
    "\"calorie_per_distance\",\n",
    "\"HeartRateVariabilitySDNN_hrs_btween\",\n",
    "\"DistanceWalkingRunning_hrs_btween\",\n",
    "\"min_endDate_max_hr\",\n",
    "\"StepCount_night_hours\",\n",
    "\"FlightsClimbed_night_hours\",\n",
    "\"distance_per_step\",\n",
    "\"day_of_year\",\n",
    "\"max_startDate_max_hr\",\n",
    "\"min_endDate_min_hr\",\n",
    "\"min_startDate_min_hr\",\n",
    "\"StepCount_hrs_btween\",\n",
    "\"BodyMassIndex\",\n",
    "\"VO2Max_hrs_btween\",\n",
    "\"doy_sin\",\n",
    "\"month\",\n",
    "\"month_cos\",\n",
    "\"avg_startDate_max_sin\",\n",
    "\"DistanceWalkingRunning\",\n",
    "\"HeadphoneAudioExposure_hrs_btween\",\n",
    "\"AppleExerciseTime_night_hours\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "apple_stand_time = pd.read_csv(\"./data/xml_export/AppleStandTime.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apple_stand_time.copy()\n",
    "\n",
    "def calculate_night_hours(df):\n",
    "    df['startDate'] = pd.to_datetime(df['startDate'])\n",
    "    df['endDate'] = pd.to_datetime(df['endDate'])\n",
    "\n",
    "    # Define the time range for night time\n",
    "    start_hour = 20  # 8 PM\n",
    "    end_hour = 8  # 8 AM\n",
    "\n",
    "    df = df[((df['startDate'].dt.hour >= start_hour) & (df['startDate'].dt.hour < 24)) |\n",
    "                ((df['endDate'].dt.hour >= 0) & (df['endDate'].dt.hour < end_hour))]\n",
    "\n",
    "    df['adjusted_startDate'] = (df['startDate'] - pd.to_timedelta('12:00:00')).dt.date # Subtract 12 hours from startDate\n",
    "    df = df.groupby(['adjusted_startDate', 'sourceName']).agg(startDate=('startDate', 'min'), endDate=('endDate', 'max'), totalValue=('value', 'sum')).reset_index()\n",
    "    df[\"night_hours\"] = (df['endDate'] - df['startDate']).dt.total_seconds() / 3600\n",
    "\n",
    "    return df\n",
    "\n",
    "df = calculate_night_hours(df)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.night_hours.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See where sourcename is Rob’s Apple Watch\n",
    "df[df['sourceName'] == \"Rob's Apple Watch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = apple_stand_time.copy()\n",
    "\n",
    "# Assuming your dataframe is named df, and has columns 'startDate' and 'endDate'\n",
    "# Ensure the columns are in datetime format\n",
    "df['startDate'] = pd.to_datetime(df['startDate'])\n",
    "df['endDate'] = pd.to_datetime(df['endDate'])\n",
    "\n",
    "# Define the time range for night time\n",
    "start_hour = 20  # 8 PM\n",
    "end_hour = 8  # 8 AM\n",
    "\n",
    "# Filter the dataframe based on the specified time range\n",
    "df_night = df[((df['startDate'].dt.hour >= start_hour) & (df['startDate'].dt.hour < 24)) |\n",
    "              ((df['endDate'].dt.hour >= 0) & (df['endDate'].dt.hour < end_hour))]\n",
    "\n",
    "\n",
    "\n",
    "# Group by the date and find min startDate and max endDate\n",
    "df_night['startDate_date'] = df_night['startDate'].dt.date\n",
    "df_night['endDate_date'] = df_night['endDate'].dt.date\n",
    "df_night_grouped = df_night.groupby(['startDate_date', 'endDate_date']).agg({'startDate': 'min', 'endDate': 'max'}).reset_index()\n",
    "\n",
    "# Hours beteween start and enddate\n",
    "df_night_grouped['hrs_btween'] = (df_night_grouped['endDate'] - df_night_grouped['startDate']).dt.total_seconds() / 3600\n",
    "\n",
    "# Display the resul\n",
    "df_night_grouped.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import POG4_Dataset\n",
    "data = POG4_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_test_split()\n",
    "data.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8*579\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = pd.concat([data.y,data.X], axis=1)\n",
    "d.to_csv(\"./regression.csv\", index=False)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in data.X.columns:\n",
    "     print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.X[\"AppleStandTime_night_hours\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Mean all columns that end in night_hours\n",
    "X = data.X[1000:].filter(like=\"night_hours\")\n",
    "y = data.y[1000:]\n",
    "\n",
    "\n",
    "model = xgb.XGBRegressor(gpu_id=0, tree_method=\"gpu_hist\", random_state=42)\n",
    "\n",
    "# Set up the cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Imputer \n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "pipeline = Pipeline(steps=[(\"imputer\", imputer), (\"scaler\", scaler), (\"model\", model)])\n",
    "\n",
    "# Perform cross-validation and calculate metrics\n",
    "cv_scores = cross_val_score(model, X, y, cv=tscv, scoring=\"neg_mean_squared_error\")\n",
    "rmse_scores = np.sqrt(-cv_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(f\"Average RMSE: {avg_rmse:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stair Ascent/Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascent = pd.read_csv(\"./data/xml_export/StairAscentSpeed.csv\", low_memory=False)\n",
    "ascent[\"type\"] = \"ascent\"\n",
    "descent = pd.read_csv(\"./data/xml_export/StairDescentSpeed.csv\", low_memory=False)\n",
    "descent[\"type\"] = \"descent\"\n",
    "\n",
    "stairs = pd.concat([ascent, descent], axis=0)\n",
    "stairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stairs[stairs[\"value\"] >= 1] # Optional?\n",
    "\n",
    "df['startDate'] = pd.to_datetime(df['startDate']).dt.tz_localize(None)\n",
    "df['endDate'] = pd.to_datetime(df['endDate']).dt.tz_localize(None)\n",
    "\n",
    "df = df.sort_values(by=['startDate', 'endDate'])\n",
    "\n",
    "# Get the date range in the dataframe\n",
    "min_date = df['startDate'].min().date()\n",
    "max_date = df['endDate'].max().date()\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each date in the range\n",
    "for date in pd.date_range(min_date, max_date):\n",
    "    # startSleep time boundaries - Based on analysis of train_detailed\n",
    "    start_day = pd.Timestamp.combine(date, pd.Timestamp('21:30:00').time())\n",
    "    end_day = pd.Timestamp.combine(date + pd.DateOffset(1), pd.Timestamp('01:30:00').time())\n",
    "    \n",
    "    # endSleep time boundaries - Based on analysis of train_detailed\n",
    "    start_night = pd.Timestamp.combine(date + pd.DateOffset(1), pd.Timestamp('06:30:00').time())\n",
    "    end_night = pd.Timestamp.combine(date + pd.DateOffset(1), pd.Timestamp('9:30:00').time())\n",
    "\n",
    "    # Filter the dataframe for max_endDate\n",
    "    mask_endDate = ((df['endDate'] >= start_day)) & ((df['endDate'] <= end_day))\n",
    "    filtered_df_endDate = df[mask_endDate]\n",
    "\n",
    "    # Filter the dataframe for min_startDate\n",
    "    mask_startDate = ((df['startDate'] >= start_night)) & ((df['startDate'] <= end_night))\n",
    "    filtered_df_startDate = df[mask_startDate]\n",
    "\n",
    "    # Find max_endDate and min_startDate\n",
    "    min_endDate = filtered_df_endDate['endDate'].min() # if not filtered_df_endDate.empty else pd.to_datetime(start_day)\n",
    "    max_endDate = filtered_df_endDate['endDate'].max() # if not filtered_df_endDate.empty else pd.to_datetime(end_day)\n",
    "    min_startDate = filtered_df_startDate['startDate'].min() # if not filtered_df_startDate.empty else pd.to_datetime(start_night)\n",
    "    max_startDate = filtered_df_startDate['startDate'].max() # if not filtered_df_startDate.empty else pd.to_datetime(end_night)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'date': date,\n",
    "        'min_endDate': min_endDate, # Min Possible Start Sleeping\n",
    "        'max_endDate': max_endDate, # Max Possible Start Sleeping\n",
    "        'min_startDate': min_startDate, # Min Possible End Sleeping\n",
    "        'max_startDate': max_startDate # Max Possible End Sleeping\n",
    "    })\n",
    "\n",
    "# Convert the results to a dataframe and return\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "# Time Differences in hours # Attempt to manually calculate sleep time - doesn't work, but still useful\n",
    "result_df[\"nhours_min_min\"] = (result_df[\"min_startDate\"] - result_df[\"min_endDate\"]).dt.total_seconds() / 3600\n",
    "result_df[\"nhours_min_max\"] = (result_df[\"min_startDate\"] - result_df[\"max_endDate\"]).dt.total_seconds() / 3600\n",
    "result_df[\"nhours_max_min\"] = (result_df[\"max_startDate\"] - result_df[\"min_endDate\"]).dt.total_seconds() / 3600\n",
    "result_df[\"nhours_max_max\"] = (result_df[\"max_startDate\"] - result_df[\"max_endDate\"]).dt.total_seconds() / 3600\n",
    "\n",
    "# Hours\n",
    "result_df[\"min_endDate_hr\"] = result_df[\"min_endDate\"].dt.hour\n",
    "result_df[\"max_endDate_hr\"] = result_df[\"max_endDate\"].dt.hour\n",
    "result_df[\"min_startDate_hr\"] = result_df[\"min_startDate\"].dt.hour\n",
    "result_df[\"max_startDate_hr\"] = result_df[\"max_startDate\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.nhours_min_max.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"./data/train_detailed.csv\", low_memory=False)\n",
    "\n",
    "# Remove rows where \"value\" is HKCategoryValueSleepAnalysisInBed\n",
    "df = df[df[\"value\"] != \"HKCategoryValueSleepAnalysisInBed\"]\n",
    "\n",
    "# Convert startDate and endDate columns to datetime objects\n",
    "df[\"startDate\"] = pd.to_datetime(df[\"startDate\"])\n",
    "df[\"endDate\"] = pd.to_datetime(df[\"endDate\"])\n",
    "\n",
    "# Create adjusted_start_date column by subtracting 12 hours from startDate\n",
    "df['adjusted_start_date'] = (df['startDate'] - pd.DateOffset(hours=12)).dt.date\n",
    "\n",
    "# Save all unique adjusted_start_date values\n",
    "unique_dates = pd.DataFrame(df[\"adjusted_start_date\"].unique(), columns=[\"adjusted_start_date\"])\n",
    "\n",
    "# Filter rows with startDate hours >= 22 or endDate hours <= 10\n",
    "df = df[(df[\"startDate\"].dt.hour >= 22) | (df[\"endDate\"].dt.hour <= 10)]\n",
    "\n",
    "# Group by adjusted_start_date and get the min startDate and max endDate\n",
    "df = df.groupby(\"adjusted_start_date\").agg({\"startDate\": \"min\", \"endDate\": \"max\"}).reset_index()\n",
    "\n",
    "# Convert startDate and endDate to hours since midnight\n",
    "df[\"startDate\"] = df[\"startDate\"].dt.hour + df[\"startDate\"].dt.minute / 60 + df[\"startDate\"].dt.second / 3600\n",
    "df[\"endDate\"] = df[\"endDate\"].dt.hour + df[\"endDate\"].dt.minute / 60 + df[\"endDate\"].dt.second / 3600\n",
    "\n",
    "# If startDate is less than 12, add 24 hours\n",
    "df.loc[df[\"startDate\"] < 12, \"startDate\"] += 24\n",
    "\n",
    "# Merge the results with the unique_dates DataFrame\n",
    "final_df = unique_dates.merge(df, on=\"adjusted_start_date\", how=\"left\").fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.endDate.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.startDate.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Creating XML data\n",
      "INFO - Creating activity data\n",
      "INFO - Missing days: 87\n",
      "INFO - Featurizing time series data\n",
      "INFO - Creating interactions...\n"
     ]
    }
   ],
   "source": [
    "from data import POG4_Dataset\n",
    "\n",
    "data = POG4_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = data.train[(data.train['date'] >= pd.to_datetime('2018-09-25').date()) & (data.train['date'] <= pd.to_datetime('2021-11-30').date())]\n",
    "\n",
    "# Using cross-validation so concat the train and test sets\n",
    "X = train.drop(['sleep_hours', 'date'], axis=1)\n",
    "y = train.sleep_hours.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slp_DistanceWalkingRunning_hrs_max_max</th>\n",
       "      <th>slp_FlightsClimbed_max_hrs_between</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>slp_StepCount_hrs_max_max</th>\n",
       "      <th>distance_per_step</th>\n",
       "      <th>hr_02:40:00</th>\n",
       "      <th>is_workday</th>\n",
       "      <th>slp_DistanceWalkingRunning_hrs_min_max</th>\n",
       "      <th>steps_23:35:00</th>\n",
       "      <th>hr_08:15:00</th>\n",
       "      <th>...</th>\n",
       "      <th>hr_03:25:00</th>\n",
       "      <th>cal_08:15:00</th>\n",
       "      <th>slp_AppleStandTime_max_hrs_between</th>\n",
       "      <th>slp_StepCount_hrs_max_min</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>hr_06:55:00</th>\n",
       "      <th>hr_06:45:00</th>\n",
       "      <th>slp_StepCount_hrs_min_max</th>\n",
       "      <th>doy_cos</th>\n",
       "      <th>steps_08:10:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>9.384167</td>\n",
       "      <td>13.061389</td>\n",
       "      <td>1</td>\n",
       "      <td>9.384167</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>6.580833</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.654722</td>\n",
       "      <td>268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.580833</td>\n",
       "      <td>-0.098820</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.772778</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.081676</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>9.441944</td>\n",
       "      <td>5.228889</td>\n",
       "      <td>3</td>\n",
       "      <td>9.441944</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>8.096667</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.441944</td>\n",
       "      <td>270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.096667</td>\n",
       "      <td>-0.064508</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>9.108333</td>\n",
       "      <td>8.928611</td>\n",
       "      <td>4</td>\n",
       "      <td>9.108333</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>8.785278</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.687500</td>\n",
       "      <td>271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.785278</td>\n",
       "      <td>-0.047321</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>9.242222</td>\n",
       "      <td>2.377778</td>\n",
       "      <td>5</td>\n",
       "      <td>9.242222</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>8.530000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.055000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.242222</td>\n",
       "      <td>272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.530000</td>\n",
       "      <td>-0.030120</td>\n",
       "      <td>23368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>7.714722</td>\n",
       "      <td>10.569167</td>\n",
       "      <td>4</td>\n",
       "      <td>7.714722</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>53.0000</td>\n",
       "      <td>True</td>\n",
       "      <td>6.845833</td>\n",
       "      <td>101.0</td>\n",
       "      <td>43.000</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.017026</td>\n",
       "      <td>5.416667</td>\n",
       "      <td>10.195000</td>\n",
       "      <td>330</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>6.845833</td>\n",
       "      <td>0.823923</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>8.337500</td>\n",
       "      <td>8.499722</td>\n",
       "      <td>5</td>\n",
       "      <td>8.290000</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>61.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>5.452222</td>\n",
       "      <td>75.0</td>\n",
       "      <td>54.000</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>10.867500</td>\n",
       "      <td>331</td>\n",
       "      <td>58.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>5.452222</td>\n",
       "      <td>0.833556</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>7.911944</td>\n",
       "      <td>10.818611</td>\n",
       "      <td>6</td>\n",
       "      <td>7.953056</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>69.5000</td>\n",
       "      <td>False</td>\n",
       "      <td>5.784444</td>\n",
       "      <td>133.0</td>\n",
       "      <td>58.000</td>\n",
       "      <td>...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.022134</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>10.292222</td>\n",
       "      <td>332</td>\n",
       "      <td>63.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>5.784444</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>8.448333</td>\n",
       "      <td>8.799167</td>\n",
       "      <td>0</td>\n",
       "      <td>8.448333</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>60.0000</td>\n",
       "      <td>True</td>\n",
       "      <td>6.228889</td>\n",
       "      <td>73.0</td>\n",
       "      <td>51.000</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>9.887500</td>\n",
       "      <td>333</td>\n",
       "      <td>52.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.228889</td>\n",
       "      <td>0.852078</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>8.246389</td>\n",
       "      <td>6.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>8.246389</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>58.7015</td>\n",
       "      <td>True</td>\n",
       "      <td>6.518611</td>\n",
       "      <td>37.5</td>\n",
       "      <td>117.808</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.115209</td>\n",
       "      <td>6.416667</td>\n",
       "      <td>8.981944</td>\n",
       "      <td>334</td>\n",
       "      <td>53.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.518611</td>\n",
       "      <td>0.860961</td>\n",
       "      <td>188.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1163 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      slp_DistanceWalkingRunning_hrs_max_max   \n",
       "1205                                9.384167  \\\n",
       "1206                                     NaN   \n",
       "1207                                9.441944   \n",
       "1208                                9.108333   \n",
       "1209                                9.242222   \n",
       "...                                      ...   \n",
       "2363                                7.714722   \n",
       "2364                                8.337500   \n",
       "2365                                7.911944   \n",
       "2366                                8.448333   \n",
       "2367                                8.246389   \n",
       "\n",
       "      slp_FlightsClimbed_max_hrs_between  day_of_week   \n",
       "1205                           13.061389            1  \\\n",
       "1206                            3.772778            2   \n",
       "1207                            5.228889            3   \n",
       "1208                            8.928611            4   \n",
       "1209                            2.377778            5   \n",
       "...                                  ...          ...   \n",
       "2363                           10.569167            4   \n",
       "2364                            8.499722            5   \n",
       "2365                           10.818611            6   \n",
       "2366                            8.799167            0   \n",
       "2367                            6.625000            1   \n",
       "\n",
       "      slp_StepCount_hrs_max_max  distance_per_step  hr_02:40:00  is_workday   \n",
       "1205                   9.384167           0.000471          NaN        True  \\\n",
       "1206                        NaN           0.000472          NaN        True   \n",
       "1207                   9.441944           0.000581          NaN        True   \n",
       "1208                   9.108333           0.000728          NaN        True   \n",
       "1209                   9.242222           0.000579          NaN       False   \n",
       "...                         ...                ...          ...         ...   \n",
       "2363                   7.714722           0.000884      53.0000        True   \n",
       "2364                   8.290000           0.000776      61.0000       False   \n",
       "2365                   7.953056           0.001030      69.5000       False   \n",
       "2366                   8.448333           0.000873      60.0000        True   \n",
       "2367                   8.246389           0.000839      58.7015        True   \n",
       "\n",
       "      slp_DistanceWalkingRunning_hrs_min_max  steps_23:35:00  hr_08:15:00   \n",
       "1205                                6.580833            20.0          NaN  \\\n",
       "1206                                     NaN           229.0          NaN   \n",
       "1207                                8.096667            33.0          NaN   \n",
       "1208                                8.785278            79.0          NaN   \n",
       "1209                                8.530000            58.0          NaN   \n",
       "...                                      ...             ...          ...   \n",
       "2363                                6.845833           101.0       43.000   \n",
       "2364                                5.452222            75.0       54.000   \n",
       "2365                                5.784444           133.0       58.000   \n",
       "2366                                6.228889            73.0       51.000   \n",
       "2367                                6.518611            37.5      117.808   \n",
       "\n",
       "      ...  hr_03:25:00  cal_08:15:00  slp_AppleStandTime_max_hrs_between   \n",
       "1205  ...          NaN      0.033635                                 NaN  \\\n",
       "1206  ...          NaN      0.020456                                 NaN   \n",
       "1207  ...          NaN      0.083612                                 NaN   \n",
       "1208  ...          NaN      0.023569                                 NaN   \n",
       "1209  ...          NaN     15.055000                                 NaN   \n",
       "...   ...          ...           ...                                 ...   \n",
       "2363  ...         53.0      0.017026                            5.416667   \n",
       "2364  ...         71.0      0.006369                            5.833333   \n",
       "2365  ...         56.0      0.022134                            6.250000   \n",
       "2366  ...         60.0      0.004065                            5.750000   \n",
       "2367  ...         57.0      0.115209                            6.416667   \n",
       "\n",
       "      slp_StepCount_hrs_max_min  day_of_year  hr_06:55:00  hr_06:45:00   \n",
       "1205                  10.654722          268          NaN          NaN  \\\n",
       "1206                        NaN          269          NaN          NaN   \n",
       "1207                   9.441944          270          NaN          NaN   \n",
       "1208                  10.687500          271          NaN          NaN   \n",
       "1209                   9.242222          272          NaN          NaN   \n",
       "...                         ...          ...          ...          ...   \n",
       "2363                  10.195000          330         49.0         49.0   \n",
       "2364                  10.867500          331         58.0         59.0   \n",
       "2365                  10.292222          332         63.0         59.0   \n",
       "2366                   9.887500          333         52.0         50.0   \n",
       "2367                   8.981944          334         53.0         52.0   \n",
       "\n",
       "      slp_StepCount_hrs_min_max   doy_cos  steps_08:10:00  \n",
       "1205                   6.580833 -0.098820            87.0  \n",
       "1206                        NaN -0.081676            44.0  \n",
       "1207                   8.096667 -0.064508            86.0  \n",
       "1208                   8.785278 -0.047321            54.0  \n",
       "1209                   8.530000 -0.030120         23368.0  \n",
       "...                         ...       ...             ...  \n",
       "2363                   6.845833  0.823923            37.0  \n",
       "2364                   5.452222  0.833556            12.0  \n",
       "2365                   5.784444  0.842942            49.0  \n",
       "2366                   6.228889  0.852078            46.0  \n",
       "2367                   6.518611  0.860961           188.0  \n",
       "\n",
       "[1163 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb.XGBRegressor().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data.X))\n",
    "print(len(data.sleep_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>21:00:00</th>\n",
       "      <th>21:05:00</th>\n",
       "      <th>21:10:00</th>\n",
       "      <th>21:15:00</th>\n",
       "      <th>21:20:00</th>\n",
       "      <th>21:25:00</th>\n",
       "      <th>21:30:00</th>\n",
       "      <th>21:35:00</th>\n",
       "      <th>21:40:00</th>\n",
       "      <th>...</th>\n",
       "      <th>08:15:00</th>\n",
       "      <th>08:20:00</th>\n",
       "      <th>08:25:00</th>\n",
       "      <th>08:30:00</th>\n",
       "      <th>08:35:00</th>\n",
       "      <th>08:40:00</th>\n",
       "      <th>08:45:00</th>\n",
       "      <th>08:50:00</th>\n",
       "      <th>08:55:00</th>\n",
       "      <th>09:00:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-04-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.0000</td>\n",
       "      <td>95.0000</td>\n",
       "      <td>105.644599</td>\n",
       "      <td>96.183333</td>\n",
       "      <td>124.206667</td>\n",
       "      <td>124.063333</td>\n",
       "      <td>137.3000</td>\n",
       "      <td>144.273333</td>\n",
       "      <td>...</td>\n",
       "      <td>112.509901</td>\n",
       "      <td>112.509901</td>\n",
       "      <td>112.509901</td>\n",
       "      <td>112.509901</td>\n",
       "      <td>112.509901</td>\n",
       "      <td>112.509901</td>\n",
       "      <td>112.509901</td>\n",
       "      <td>112.509901</td>\n",
       "      <td>112.509901</td>\n",
       "      <td>112.509901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>2023-03-13</td>\n",
       "      <td>84.0</td>\n",
       "      <td>86.0000</td>\n",
       "      <td>86.0000</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>83.250000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.2500</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>54.500000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.250000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>76.0</td>\n",
       "      <td>73.5000</td>\n",
       "      <td>73.5000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>73.750000</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>70.5000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>96.500000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>94.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0856</td>\n",
       "      <td>60.0856</td>\n",
       "      <td>60.085600</td>\n",
       "      <td>60.085600</td>\n",
       "      <td>60.085600</td>\n",
       "      <td>60.085600</td>\n",
       "      <td>60.0856</td>\n",
       "      <td>60.085600</td>\n",
       "      <td>...</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>96.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>2023-03-16</td>\n",
       "      <td>64.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>60.250000</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>2023-03-17</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.0000</td>\n",
       "      <td>88.0000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>61.5000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>920 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "time        date  21:00:00  21:05:00  21:10:00    21:15:00   21:20:00   \n",
       "0     2015-04-20       NaN       NaN       NaN         NaN        NaN  \\\n",
       "1     2015-04-21       NaN   95.0000   95.0000  105.644599  96.183333   \n",
       "2     2015-04-22       NaN       NaN       NaN         NaN        NaN   \n",
       "3     2015-04-23       NaN       NaN       NaN         NaN        NaN   \n",
       "4     2015-04-24       NaN       NaN       NaN         NaN        NaN   \n",
       "..           ...       ...       ...       ...         ...        ...   \n",
       "915   2023-03-13      84.0   86.0000   86.0000   95.500000  83.250000   \n",
       "916   2023-03-14      76.0   73.5000   73.5000   77.000000  73.750000   \n",
       "917   2023-03-15       NaN   60.0856   60.0856   60.085600  60.085600   \n",
       "918   2023-03-16      64.0   76.0000   76.0000   76.000000  76.000000   \n",
       "919   2023-03-17      61.0   60.0000   88.0000  116.000000  57.000000   \n",
       "\n",
       "time    21:25:00    21:30:00  21:35:00    21:40:00  ...    08:15:00   \n",
       "0            NaN         NaN       NaN         NaN  ...         NaN  \\\n",
       "1     124.206667  124.063333  137.3000  144.273333  ...  112.509901   \n",
       "2            NaN         NaN       NaN         NaN  ...         NaN   \n",
       "3            NaN         NaN       NaN         NaN  ...         NaN   \n",
       "4            NaN         NaN       NaN         NaN  ...         NaN   \n",
       "..           ...         ...       ...         ...  ...         ...   \n",
       "915    71.000000   76.000000   76.2500   76.500000  ...   56.000000   \n",
       "916    70.500000   70.500000   70.5000   70.000000  ...   78.000000   \n",
       "917    60.085600   60.085600   60.0856   60.085600  ...   99.000000   \n",
       "918    76.000000   76.000000   76.0000   76.000000  ...   67.000000   \n",
       "919    60.000000   63.000000   61.5000   74.000000  ...   70.000000   \n",
       "\n",
       "time    08:20:00    08:25:00    08:30:00    08:35:00    08:40:00    08:45:00   \n",
       "0            NaN         NaN         NaN         NaN         NaN         NaN  \\\n",
       "1     112.509901  112.509901  112.509901  112.509901  112.509901  112.509901   \n",
       "2            NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3            NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "4            NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "..           ...         ...         ...         ...         ...         ...   \n",
       "915    54.500000   53.000000   54.000000   52.000000   52.250000   52.500000   \n",
       "916    80.000000   85.000000   91.500000   94.000000   96.500000   89.000000   \n",
       "917   115.000000   99.000000   95.000000   97.000000  100.000000   99.000000   \n",
       "918    66.500000   66.000000   64.000000   59.000000   60.000000   60.250000   \n",
       "919    70.000000   70.000000   70.000000   70.000000   70.000000   70.000000   \n",
       "\n",
       "time    08:50:00    08:55:00    09:00:00  \n",
       "0            NaN         NaN         NaN  \n",
       "1     112.509901  112.509901  112.509901  \n",
       "2            NaN         NaN         NaN  \n",
       "3            NaN         NaN         NaN  \n",
       "4            NaN         NaN         NaN  \n",
       "..           ...         ...         ...  \n",
       "915    49.000000   51.000000   52.000000  \n",
       "916    92.500000   96.000000   94.000000  \n",
       "917   100.000000   96.000000   96.000000  \n",
       "918    60.500000   61.000000   61.000000  \n",
       "919    70.000000   70.000000   70.000000  \n",
       "\n",
       "[920 rows x 146 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "def process_heart_rate_data(filename, interval_minutes):\n",
    "    df = pd.read_csv(filename, low_memory=False)\n",
    "\n",
    "    # Keep only startDate, endDate, and value columns\n",
    "    df = df[[\"startDate\", \"endDate\", \"value\"]]\n",
    "\n",
    "    df['startDate'] = pd.to_datetime(df['startDate'])\n",
    "    df['startDate'] = df['startDate'] - pd.Timedelta(hours=12)\n",
    "\n",
    "    df['date'] = df['startDate'].dt.date\n",
    "\n",
    "    def round_time_to_nearest_interval(time):\n",
    "        minutes = (time.hour * 60) + time.minute\n",
    "        rounded_minutes = round(minutes / interval_minutes) * interval_minutes\n",
    "        return dt.time(hour=(rounded_minutes // 60) % 24, minute=rounded_minutes % 60)\n",
    "\n",
    "    df['time'] = df['startDate'].dt.time.map(round_time_to_nearest_interval)\n",
    "\n",
    "    df_grouped = df.groupby(['date', 'time'])['value'].mean().reset_index()\n",
    "\n",
    "    df_pivot = df_grouped.pivot_table(index='date', columns='time', values='value').reset_index()\n",
    "\n",
    "    df_pivot.set_index('date', inplace=True)\n",
    "\n",
    "    time_start = dt.time(hour=9)\n",
    "    time_end = dt.time(hour=21)\n",
    "    df_filtered = df_pivot.loc[:, (df_pivot.columns >= time_start) & (df_pivot.columns <= time_end)]\n",
    "\n",
    "\n",
    "    def add_12_hours_to_time(time_obj):\n",
    "        datetime_obj = dt.datetime.combine(dt.date(1, 1, 1), time_obj)\n",
    "        datetime_obj += dt.timedelta(hours=12)\n",
    "        return datetime_obj.time()\n",
    "\n",
    "    df_filtered = df_filtered.rename(columns=add_12_hours_to_time)\n",
    "    df_filtered.iloc[:, 1:] = df_filtered.iloc[:, 1:].interpolate(axis=1).ffill(axis=1).bfill(axis=1)\n",
    "    df_filtered = df_filtered.reset_index()\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "filename = \"./data/xml_export/HeartRate.csv\"\n",
    "interval_minutes = 5\n",
    "\n",
    "df_filtered = process_heart_rate_data(filename, interval_minutes)\n",
    "\n",
    "\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(920, 146)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pog4-sleep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
